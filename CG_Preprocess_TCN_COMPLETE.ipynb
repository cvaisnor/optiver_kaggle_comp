{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":6640818,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Conv1D, Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import BatchNormalization\nfrom itertools import groupby\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport gc\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import combinations\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom warnings import simplefilter\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"id":"dhW1edGiuwGS","execution":{"iopub.status.busy":"2023-11-13T10:35:53.219293Z","iopub.execute_input":"2023-11-13T10:35:53.219910Z","iopub.status.idle":"2023-11-13T10:35:58.788548Z","shell.execute_reply.started":"2023-11-13T10:35:53.219856Z","shell.execute_reply":"2023-11-13T10:35:58.786924Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n/kaggle/input/optiver-trading-at-the-close/train.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n/kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\nrevealed_targets = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\ntest = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:35:58.796731Z","iopub.execute_input":"2023-11-13T10:35:58.797178Z","iopub.status.idle":"2023-11-13T10:36:13.341760Z","shell.execute_reply.started":"2023-11-13T10:35:58.797141Z","shell.execute_reply":"2023-11-13T10:36:13.340294Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"id":"1fyIzl0HyvfW","execution":{"iopub.status.busy":"2023-11-13T10:36:13.343310Z","iopub.execute_input":"2023-11-13T10:36:13.343702Z","iopub.status.idle":"2023-11-13T10:36:13.380015Z","shell.execute_reply.started":"2023-11-13T10:36:13.343670Z","shell.execute_reply":"2023-11-13T10:36:13.378643Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n0               0        0                  0      3180602.69   \n1               1        0                  0       166603.91   \n2               2        0                  0       302879.87   \n3               3        0                  0     11917682.27   \n4               4        0                  0       447549.96   \n...           ...      ...                ...             ...   \n5237975       195      480                540      2440722.89   \n5237976       196      480                540       349510.47   \n5237977       197      480                540            0.00   \n5237978       198      480                540      1000898.84   \n5237979       199      480                540      1884285.71   \n\n         imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n0                              1         0.999812   13380276.64        NaN   \n1                             -1         0.999896    1642214.25        NaN   \n2                             -1         0.999561    1819368.03        NaN   \n3                             -1         1.000171   18389745.62        NaN   \n4                             -1         0.999532   17860614.95        NaN   \n...                          ...              ...           ...        ...   \n5237975                       -1         1.000317   28280361.74   0.999734   \n5237976                       -1         1.000643    9187699.11   1.000129   \n5237977                        0         0.995789   12725436.10   0.995789   \n5237978                        1         0.999210   94773271.05   0.999210   \n5237979                       -1         1.002129   24073677.32   1.000859   \n\n         near_price  bid_price   bid_size  ask_price   ask_size       wap  \\\n0               NaN   0.999812   60651.50   1.000026    8493.03  1.000000   \n1               NaN   0.999896    3233.04   1.000660   20605.09  1.000000   \n2               NaN   0.999403   37956.00   1.000298   18995.00  1.000000   \n3               NaN   0.999999    2324.90   1.000214  479032.40  1.000000   \n4               NaN   0.999394   16485.54   1.000016     434.10  1.000000   \n...             ...        ...        ...        ...        ...       ...   \n5237975    0.999734   1.000317   32257.04   1.000434  319862.40  1.000328   \n5237976    1.000386   1.000643  205108.40   1.000900   93393.07  1.000819   \n5237977    0.995789   0.995789   16790.66   0.995883  180038.32  0.995797   \n5237978    0.999210   0.998970  125631.72   0.999210  669893.00  0.999008   \n5237979    1.001494   1.002129  250081.44   1.002447  300167.56  1.002274   \n\n           target  time_id       row_id  \n0       -3.029704        0        0_0_0  \n1       -5.519986        0        0_0_1  \n2       -8.389950        0        0_0_2  \n3       -4.010200        0        0_0_3  \n4       -7.349849        0        0_0_4  \n...           ...      ...          ...  \n5237975  2.310276    26454  480_540_195  \n5237976 -8.220077    26454  480_540_196  \n5237977  1.169443    26454  480_540_197  \n5237978 -1.540184    26454  480_540_198  \n5237979 -6.530285    26454  480_540_199  \n\n[5237980 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_id</th>\n      <th>date_id</th>\n      <th>seconds_in_bucket</th>\n      <th>imbalance_size</th>\n      <th>imbalance_buy_sell_flag</th>\n      <th>reference_price</th>\n      <th>matched_size</th>\n      <th>far_price</th>\n      <th>near_price</th>\n      <th>bid_price</th>\n      <th>bid_size</th>\n      <th>ask_price</th>\n      <th>ask_size</th>\n      <th>wap</th>\n      <th>target</th>\n      <th>time_id</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3180602.69</td>\n      <td>1</td>\n      <td>0.999812</td>\n      <td>13380276.64</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999812</td>\n      <td>60651.50</td>\n      <td>1.000026</td>\n      <td>8493.03</td>\n      <td>1.000000</td>\n      <td>-3.029704</td>\n      <td>0</td>\n      <td>0_0_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>166603.91</td>\n      <td>-1</td>\n      <td>0.999896</td>\n      <td>1642214.25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999896</td>\n      <td>3233.04</td>\n      <td>1.000660</td>\n      <td>20605.09</td>\n      <td>1.000000</td>\n      <td>-5.519986</td>\n      <td>0</td>\n      <td>0_0_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>302879.87</td>\n      <td>-1</td>\n      <td>0.999561</td>\n      <td>1819368.03</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999403</td>\n      <td>37956.00</td>\n      <td>1.000298</td>\n      <td>18995.00</td>\n      <td>1.000000</td>\n      <td>-8.389950</td>\n      <td>0</td>\n      <td>0_0_2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11917682.27</td>\n      <td>-1</td>\n      <td>1.000171</td>\n      <td>18389745.62</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999999</td>\n      <td>2324.90</td>\n      <td>1.000214</td>\n      <td>479032.40</td>\n      <td>1.000000</td>\n      <td>-4.010200</td>\n      <td>0</td>\n      <td>0_0_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>447549.96</td>\n      <td>-1</td>\n      <td>0.999532</td>\n      <td>17860614.95</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999394</td>\n      <td>16485.54</td>\n      <td>1.000016</td>\n      <td>434.10</td>\n      <td>1.000000</td>\n      <td>-7.349849</td>\n      <td>0</td>\n      <td>0_0_4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5237975</th>\n      <td>195</td>\n      <td>480</td>\n      <td>540</td>\n      <td>2440722.89</td>\n      <td>-1</td>\n      <td>1.000317</td>\n      <td>28280361.74</td>\n      <td>0.999734</td>\n      <td>0.999734</td>\n      <td>1.000317</td>\n      <td>32257.04</td>\n      <td>1.000434</td>\n      <td>319862.40</td>\n      <td>1.000328</td>\n      <td>2.310276</td>\n      <td>26454</td>\n      <td>480_540_195</td>\n    </tr>\n    <tr>\n      <th>5237976</th>\n      <td>196</td>\n      <td>480</td>\n      <td>540</td>\n      <td>349510.47</td>\n      <td>-1</td>\n      <td>1.000643</td>\n      <td>9187699.11</td>\n      <td>1.000129</td>\n      <td>1.000386</td>\n      <td>1.000643</td>\n      <td>205108.40</td>\n      <td>1.000900</td>\n      <td>93393.07</td>\n      <td>1.000819</td>\n      <td>-8.220077</td>\n      <td>26454</td>\n      <td>480_540_196</td>\n    </tr>\n    <tr>\n      <th>5237977</th>\n      <td>197</td>\n      <td>480</td>\n      <td>540</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.995789</td>\n      <td>12725436.10</td>\n      <td>0.995789</td>\n      <td>0.995789</td>\n      <td>0.995789</td>\n      <td>16790.66</td>\n      <td>0.995883</td>\n      <td>180038.32</td>\n      <td>0.995797</td>\n      <td>1.169443</td>\n      <td>26454</td>\n      <td>480_540_197</td>\n    </tr>\n    <tr>\n      <th>5237978</th>\n      <td>198</td>\n      <td>480</td>\n      <td>540</td>\n      <td>1000898.84</td>\n      <td>1</td>\n      <td>0.999210</td>\n      <td>94773271.05</td>\n      <td>0.999210</td>\n      <td>0.999210</td>\n      <td>0.998970</td>\n      <td>125631.72</td>\n      <td>0.999210</td>\n      <td>669893.00</td>\n      <td>0.999008</td>\n      <td>-1.540184</td>\n      <td>26454</td>\n      <td>480_540_198</td>\n    </tr>\n    <tr>\n      <th>5237979</th>\n      <td>199</td>\n      <td>480</td>\n      <td>540</td>\n      <td>1884285.71</td>\n      <td>-1</td>\n      <td>1.002129</td>\n      <td>24073677.32</td>\n      <td>1.000859</td>\n      <td>1.001494</td>\n      <td>1.002129</td>\n      <td>250081.44</td>\n      <td>1.002447</td>\n      <td>300167.56</td>\n      <td>1.002274</td>\n      <td>-6.530285</td>\n      <td>26454</td>\n      <td>480_540_199</td>\n    </tr>\n  </tbody>\n</table>\n<p>5237980 rows × 17 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\ndef apply_differencing(df, columns_to_difference):\n    \"\"\"\n    Apply pandas diff() method to specified columns in a DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to process.\n    columns_to_difference (list of str): List of column names to apply differencing.\n\n    Returns:\n    pandas.DataFrame: DataFrame with differenced columns.\n    \"\"\"\n    for column in columns_to_difference:\n        if column in df.columns:\n            df[f'{column}_diff'] = df[column].diff()\n        else:\n            print(f\"Column {column} not found in DataFrame.\")\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:36:13.383015Z","iopub.execute_input":"2023-11-13T10:36:13.383538Z","iopub.status.idle":"2023-11-13T10:36:13.391050Z","shell.execute_reply.started":"2023-11-13T10:36:13.383503Z","shell.execute_reply":"2023-11-13T10:36:13.389674Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def create_lagged_features(df, columns, n_lags, fill_method='ffill'):\n\n    lagged_df = df.copy()\n    for col in columns:\n        # lagged features for each column\n        for n in range(1, n_lags + 1):\n            lagged_col_name = f'{col}_lag{n}'\n            lagged_df[lagged_col_name] = df[col].shift(n)\n            # fill method after each lagged column\n            if fill_method == 'ffill':\n                lagged_df[lagged_col_name].fillna(method='ffill', inplace=True)\n            elif fill_method == 'zero':\n                lagged_df[lagged_col_name].fillna(0, inplace=True)\n\n    return lagged_df","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:36:13.392613Z","iopub.execute_input":"2023-11-13T10:36:13.392976Z","iopub.status.idle":"2023-11-13T10:36:13.405483Z","shell.execute_reply.started":"2023-11-13T10:36:13.392943Z","shell.execute_reply":"2023-11-13T10:36:13.404491Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\ndef impute_missing_values(df, columns_to_impute, max_iter=10, random_state=0):\n    \"\"\"\n    Impute missing values in specified columns of a DataFrame using IterativeImputer.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to process.\n    columns_to_impute (list of str): List of column names to apply imputation.\n    max_iter (int): Maximum number of imputation iterations.\n    random_state (int): Random state for reproducibility.\n\n    Returns:\n    pandas.DataFrame: DataFrame with imputed columns.\n    \"\"\"\n    # Check if columns_to_impute are in df\n    columns_to_impute = [col for col in columns_to_impute if col in df.columns]\n\n    # Select the columns for imputation\n    columns_data = df[columns_to_impute]\n\n    # Check if there are any missing values and if the columns are numeric\n    if columns_data.isna().any().any() and columns_data.select_dtypes(include='number').shape[1] == len(columns_to_impute):\n        imputer = IterativeImputer(max_iter=max_iter, random_state=random_state)\n\n        # Apply imputation\n        imputed_data = imputer.fit_transform(columns_data)\n        imputed_df = pd.DataFrame(imputed_data, columns=columns_to_impute, index=df.index)\n\n        # Update the original DataFrame\n        df.update(imputed_df)\n\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:36:13.406926Z","iopub.execute_input":"2023-11-13T10:36:13.407276Z","iopub.status.idle":"2023-11-13T10:36:13.417849Z","shell.execute_reply.started":"2023-11-13T10:36:13.407245Z","shell.execute_reply":"2023-11-13T10:36:13.416635Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef preprocess_and_interpolate(df, columns_to_interpolate, start_time, freq='T', order=3):\n    \"\"\"\n    Preprocess and interpolate specified columns of a DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): DataFrame to process.\n    columns_to_interpolate (list of str): Columns to apply interpolation.\n    start_time (str or datetime): Start time for generating the time range.\n    freq (str): Frequency for resampling. Default is 'T' for minute-wise.\n    order (int): Order of spline interpolation. Default is 3.\n\n    Returns:\n    pandas.DataFrame: Processed and interpolated DataFrame.\n    \"\"\"\n    # Generate a time range and map 'date_id' to the time_range\n    time_range = pd.date_range(start=start_time, periods=len(df), freq=freq)\n    df['actual_time'] = df['date_id'].apply(lambda x: time_range[x])\n\n    # Set the timestamp as the index\n    df.set_index('actual_time', inplace=True)\n\n    # Initialize an empty DataFrame to store the results\n    resampled_dfs = []\n\n    # Group by 'stock_id' and process each group\n    for stock_id, group in df.groupby('stock_id'):\n        # Handle duplicates\n        if group.index.duplicated().any():\n            group = group[~group.index.duplicated(keep='first')]\n\n        # Resample the data\n        group_resampled = group.resample(freq).asfreq()\n\n        # Interpolate for each column\n        for column in columns_to_interpolate:\n            group_resampled[column] = group_resampled[column].interpolate(method='spline', order=order)\n        \n        # Add the stock_id back to the DataFrame\n        group_resampled['stock_id'] = stock_id\n\n        # Append the processed group to the list\n        resampled_dfs.append(group_resampled)\n\n    # Concatenate all the processed groups\n    df_resampled = pd.concat(resampled_dfs)\n    df_resampled.reset_index(inplace=True)\n\n    return df_resampled\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:36:13.419297Z","iopub.execute_input":"2023-11-13T10:36:13.419674Z","iopub.status.idle":"2023-11-13T10:36:13.475026Z","shell.execute_reply.started":"2023-11-13T10:36:13.419644Z","shell.execute_reply":"2023-11-13T10:36:13.473714Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef preprocess_data(df, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, fill_method='ffill', start_time=None, freq='T', order=3):\n    \"\"\"\n    Apply a series of preprocessing steps: differencing, imputing, interpolating, and creating lagged features.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to process.\n    columns_to_difference (list of str): Columns to apply differencing.\n    columns_to_impute (list of str): Columns to apply imputation.\n    columns_to_interpolate (list of str): Columns to apply interpolation.\n    lagged_columns (list of str): Columns to create lagged features for.\n    n_lags (int): Number of lags to create.\n    fill_method (str): Method for filling NaN values in lagged features.\n    start_time (str or datetime): Start time for time range generation in interpolation.\n    freq (str): Frequency for resampling in interpolation.\n    order (int): Order of spline interpolation.\n\n    Returns:\n    pandas.DataFrame: The processed DataFrame.\n    \"\"\"\n    # Apply differencing\n    df = apply_differencing(df, columns_to_difference)\n    df.dropna(subset=columns_to_difference)\n\n    # Impute missing values\n    df = impute_missing_values(df, columns_to_impute)\n\n    # Interpolate\n    if start_time:\n        df = preprocess_and_interpolate(df, columns_to_interpolate, start_time, freq, order)\n\n    # Create lagged features\n    df = create_lagged_features(df, lagged_columns, n_lags, fill_method)\n\n    return df\n\n# processed_df = preprocess_data(train, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, fill_method='ffill', start_time=start_time, freq='T', order=3)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:36:13.476910Z","iopub.execute_input":"2023-11-13T10:36:13.477355Z","iopub.status.idle":"2023-11-13T10:36:13.490338Z","shell.execute_reply.started":"2023-11-13T10:36:13.477317Z","shell.execute_reply":"2023-11-13T10:36:13.489316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"columns_to_difference = ['bid_price', 'ask_price']\ncolumns_to_impute = ['far_price', 'near_price']\ncolumns_to_interpolate = ['bid_price', 'ask_price']\nlagged_columns = ['far_price', 'near_price']\nn_lags = 5\nstart_time = pd.to_datetime('2023-01-01 08:00:00')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:36:13.492102Z","iopub.execute_input":"2023-11-13T10:36:13.492695Z","iopub.status.idle":"2023-11-13T10:36:13.510259Z","shell.execute_reply.started":"2023-11-13T10:36:13.492656Z","shell.execute_reply":"2023-11-13T10:36:13.508848Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def preprocess_train_data(df, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, fill_method, start_time, freq, order):\n    \"\"\"\n    Apply a full preprocessing pipeline to the DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): DataFrame to process.\n    columns_to_difference (list of str): Columns to apply differencing.\n    columns_to_impute (list of str): Columns to apply imputation.\n    columns_to_interpolate (list of str): Columns to apply interpolation.\n    lagged_columns (list of str): Columns to create lagged features for.\n    n_lags (int): Number of lags to create.\n    fill_method (str): Method for filling NaN values in lagged features.\n    start_time (datetime): Start time for generating the time range.\n    freq (str): Frequency for resampling in interpolation.\n    order (int): Order of spline interpolation.\n\n    Returns:\n    numpy.ndarray: Scaled feature array.\n    numpy.ndarray: Target array (if available).\n    \"\"\"\n    # Apply differencing, imputation, interpolation, and create lagged features\n    df = preprocess_data(df, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, fill_method, start_time, freq, order)\n\n    # Replace inf values with NaN\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # Drop rows with NaN values\n    df.dropna(inplace=True)\n\n    # Check if 'target' column exists and separate it\n    if 'target' in df.columns:\n        y = df['target'].values\n        df = df.drop(['target'], axis=1)\n    else:\n        y = None\n\n    # Drop 'actual_time' column if it exists\n    if 'actual_time' in df.columns:\n        df = df.drop(['actual_time'], axis=1)\n\n    # Convert DataFrame to numpy array and scale the features\n    X = df.to_numpy(dtype='float32')\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    return X_scaled, y\n\n# Example usage\nprocessed_X, processed_y = preprocess_train_data(train, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, 'ffill', start_time, 'T', 3)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:36:13.512660Z","iopub.execute_input":"2023-11-13T10:36:13.513238Z","iopub.status.idle":"2023-11-13T10:37:03.180809Z","shell.execute_reply.started":"2023-11-13T10:36:13.513192Z","shell.execute_reply":"2023-11-13T10:37:03.179572Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def preprocess_test_data(df, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, fill_method, start_time, freq, order):\n    \"\"\"\n    Apply preprocessing steps to the test DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): DataFrame to process.\n    columns_to_difference (list of str): Columns to apply differencing.\n    columns_to_impute (list of str): Columns to apply imputation.\n    columns_to_interpolate (list of str): Columns to apply interpolation.\n    lagged_columns (list of str): Columns to create lagged features for.\n    n_lags (int): Number of lags to create.\n    fill_method (str): Method for filling NaN values in lagged features.\n    start_time (datetime): Start time for generating the time range.\n    freq (str): Frequency for resampling in interpolation.\n    order (int): Order of spline interpolation.\n\n    Returns:\n    numpy.ndarray: Scaled feature array.\n    \"\"\"\n    # Apply differencing, imputation, interpolation, and create lagged features\n    df = preprocess_data(df, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, fill_method, start_time, freq, order)\n\n    # Replace inf values with NaN\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    # Drop rows with NaN values\n    df.dropna(inplace=True)\n\n    # Drop 'actual_time' column if it exists\n    if 'actual_time' in df.columns:\n        df = df.drop(['actual_time'], axis=1)\n\n    # Convert DataFrame to numpy array and scale the features\n    X = df.to_numpy(dtype='float32')\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    return X_scaled\n\n# Example usage\nprocessed_test_X = preprocess_test_data(test, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, 'ffill', start_time, 'T', 3)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:37:03.182534Z","iopub.execute_input":"2023-11-13T10:37:03.182868Z","iopub.status.idle":"2023-11-13T10:37:04.264931Z","shell.execute_reply.started":"2023-11-13T10:37:03.182839Z","shell.execute_reply":"2023-11-13T10:37:04.263809Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"processed_X","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:37:04.266517Z","iopub.execute_input":"2023-11-13T10:37:04.267308Z","iopub.status.idle":"2023-11-13T10:37:04.276080Z","shell.execute_reply.started":"2023-11-13T10:37:04.267271Z","shell.execute_reply":"2023-11-13T10:37:04.274309Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[-1.7158246, -1.707438 ,  0.       , ...,  0.       ,  0.       ,\n         0.       ],\n       [-1.7158246, -1.7002189,  0.       , ...,  0.       ,  0.       ,\n         0.       ],\n       [-1.7158246, -1.693    ,  0.       , ...,  0.       ,  0.       ,\n         0.       ],\n       ...,\n       [ 1.7229621,  1.7071792,  0.       , ...,  0.       ,  0.       ,\n         0.       ],\n       [ 1.7229621,  1.7143983,  0.       , ...,  0.       ,  0.       ,\n         0.       ],\n       [ 1.7229621,  1.7216173,  0.       , ...,  0.       ,  0.       ,\n         0.       ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"processed_test_X","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:37:04.281235Z","iopub.execute_input":"2023-11-13T10:37:04.282642Z","iopub.status.idle":"2023-11-13T10:37:04.291182Z","shell.execute_reply.started":"2023-11-13T10:37:04.282589Z","shell.execute_reply":"2023-11-13T10:37:04.289583Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[-1.7349446 ,  1.2221756 ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.7174786 , -1.2262908 ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.7174786 , -0.00205753,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 1.7233202 , -1.2262908 ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 1.7233202 , -0.00205753,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 1.7233202 ,  1.2221756 ,  0.        , ...,  0.        ,\n         0.        ,  0.        ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Conv1D, Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import BatchNormalization\n\n\nclass TemporalBlock(Layer):\n    def __init__(self, n_filters, kernel_size, dilation_rate):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = Conv1D(\n            filters=n_filters,\n            kernel_size=kernel_size,\n            padding='causal',\n            dilation_rate=dilation_rate\n        )\n        self.batch_norm1 = BatchNormalization()\n        self.conv2 = Conv1D(\n            filters=n_filters,\n            kernel_size=kernel_size,\n            padding='causal',\n            dilation_rate=dilation_rate\n        )\n        self.batch_norm2 = BatchNormalization()\n        self.dropout = Dropout(0.2)\n    \n    def call(self, inputs, training=False):\n        x = self.conv1(inputs)\n        x = self.batch_norm1(x, training=training)\n        x = tf.nn.relu(x)\n        x = self.dropout(x, training=training)\n        x = self.conv2(x)\n        x = self.batch_norm2(x, training=training)\n        x = tf.nn.relu(x)\n        x = self.dropout(x, training=training)\n        return x + inputs  # Residual connection\n\nclass TCN(Layer):\n    def __init__(self, n_filters, kernel_size, dilation_rates):\n        super(TCN, self).__init__()\n        self.blocks = [TemporalBlock(n_filters, kernel_size, dr) for dr in dilation_rates]\n\n    def call(self, inputs, training=False):\n        x = inputs\n        for block in self.blocks:\n            x = block(x, training=training)\n        return x\n\n# Define the TCN model\nn_filters = 64\nkernel_size = 3\ndilation_rates = [1, 2, 4, 8]  # exadilation rates\n\ninput_layer = Input(shape=(None, 1))  # univariate time series; change 1 to n_features if multivariate\ntcn_layer = TCN(n_filters, kernel_size, dilation_rates)(input_layer)\noutput_layer = Dense(1)(tcn_layer)  # regression task\n\nm = Model(inputs=input_layer, outputs=output_layer)\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipvalue=1.0)\nm.compile(optimizer=optimizer, loss='mae')\nm.summary()\nearly_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\nm.fit(processed_X, processed_y, epochs=1, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:37:04.293279Z","iopub.execute_input":"2023-11-13T10:37:04.293859Z","iopub.status.idle":"2023-11-13T10:39:06.511867Z","shell.execute_reply.started":"2023-11-13T10:37:04.293807Z","shell.execute_reply":"2023-11-13T10:39:06.510414Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, None, 1)]         0         \n                                                                 \n tcn (TCN)                   (None, None, 64)          88768     \n                                                                 \n dense (Dense)               (None, None, 1)           65        \n                                                                 \n=================================================================\nTotal params: 88,833\nTrainable params: 87,809\nNon-trainable params: 1,024\n_________________________________________________________________\n2976/2976 [==============================] - 121s 39ms/step - loss: 8.0163\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7e379eb56260>"},"metadata":{}}]},{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()","metadata":{"id":"dtgufAI0_Mai","execution":{"iopub.status.busy":"2023-11-13T10:39:06.513632Z","iopub.execute_input":"2023-11-13T10:39:06.514750Z","iopub.status.idle":"2023-11-13T10:39:06.525876Z","shell.execute_reply.started":"2023-11-13T10:39:06.514704Z","shell.execute_reply":"2023-11-13T10:39:06.524959Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"counter = 0\nfor (test_batch, revealed_targets, sample_prediction) in iter_test:\n    if counter == 0:\n        print(test_batch.head(3))\n        print(revealed_targets.head(3))\n        print(sample_prediction.head(3))\n    X_feat = preprocess_test_data(test_batch, columns_to_difference, columns_to_impute, columns_to_interpolate, lagged_columns, n_lags, 'ffill', start_time, 'T', 3)\n    sample_prediction['target'] = m.predict(X_feat)\n    env.predict(sample_prediction)\n    counter += 1\n","metadata":{"id":"n9l3PJ_j_SHl","execution":{"iopub.status.busy":"2023-11-13T10:39:07.197220Z","iopub.status.idle":"2023-11-13T10:39:07.197651Z","shell.execute_reply.started":"2023-11-13T10:39:07.197462Z","shell.execute_reply":"2023-11-13T10:39:07.197483Z"},"trusted":true},"execution_count":null,"outputs":[]}]}