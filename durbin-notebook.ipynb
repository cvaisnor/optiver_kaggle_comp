{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optiver Trading At The Close\n",
    "\n",
    "[Competition](https://www.kaggle.com/competitions/optiver-trading-at-the-close)\n",
    "\n",
    "[GitHub Repository](https://github.com/cvaisnor/DNN-Final-Project)\n",
    "\n",
    "[Google Slides Presentation](https://docs.google.com/presentation/d/1Xc5F1_NveFi1il3GqHej2aqVmklR_jZU4kx6ZJhiDEM/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "kaggle_environment = False # True if running on Kaggle, don't forget to add the dataset!\n",
    "\n",
    "if kaggle_environment:\n",
    "    data_path = '/kaggle/input/'\n",
    "else:\n",
    "    data_path = 'kaggle/input/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle/input/optiver-trading-at-the-close/.gitkeep\n",
      "kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n",
      "kaggle/input/optiver-trading-at-the-close/train.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/.ipynb_checkpoints/test-checkpoint.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/.ipynb_checkpoints/sample_submission-checkpoint.csv\n",
      "kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n",
      "kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk(data_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = pd.read_csv(data_path + 'optiver-trading-at-the-close/train.csv')\n",
    "revealed_targets = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\n",
    "test = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/test.csv')\n",
    "sample_submission = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5237980 entries, 0 to 5237979\n",
      "Data columns (total 17 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   stock_id                 int64  \n",
      " 1   date_id                  int64  \n",
      " 2   seconds_in_bucket        int64  \n",
      " 3   imbalance_size           float64\n",
      " 4   imbalance_buy_sell_flag  int64  \n",
      " 5   reference_price          float64\n",
      " 6   matched_size             float64\n",
      " 7   far_price                float64\n",
      " 8   near_price               float64\n",
      " 9   bid_price                float64\n",
      " 10  bid_size                 float64\n",
      " 11  ask_price                float64\n",
      " 12  ask_size                 float64\n",
      " 13  wap                      float64\n",
      " 14  target                   float64\n",
      " 15  time_id                  int64  \n",
      " 16  row_id                   object \n",
      "dtypes: float64(11), int64(5), object(1)\n",
      "memory usage: 679.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33000 entries, 0 to 32999\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   stock_id                 33000 non-null  int64  \n",
      " 1   date_id                  33000 non-null  int64  \n",
      " 2   seconds_in_bucket        33000 non-null  int64  \n",
      " 3   imbalance_size           33000 non-null  float64\n",
      " 4   imbalance_buy_sell_flag  33000 non-null  int64  \n",
      " 5   reference_price          33000 non-null  float64\n",
      " 6   matched_size             33000 non-null  float64\n",
      " 7   far_price                14771 non-null  float64\n",
      " 8   near_price               15000 non-null  float64\n",
      " 9   bid_price                33000 non-null  float64\n",
      " 10  bid_size                 33000 non-null  float64\n",
      " 11  ask_price                33000 non-null  float64\n",
      " 12  ask_size                 33000 non-null  float64\n",
      " 13  wap                      33000 non-null  float64\n",
      " 14  time_id                  33000 non-null  int64  \n",
      " 15  row_id                   33000 non-null  object \n",
      "dtypes: float64(10), int64(5), object(1)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "original_train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "split_idx = int(len(original_train) * split_ratio)\n",
    "\n",
    "train = original_train.iloc[:split_idx]\n",
    "test = original_train.iloc[split_idx:]\n",
    "\n",
    "y = train['target'].values\n",
    "y_test = test['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:79: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "<timed exec>:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:79: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "<timed exec>:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 2.97 s, total: 15.7 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\n",
    "\n",
    "class LogFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            X[f'{column}_log'] = np.log1p(X[column].clip(lower=0.00001))\n",
    "        return X\n",
    "\n",
    "class WapLagFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, shift_sizes):\n",
    "        self.shift_sizes = shift_sizes\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for shift_size in self.shift_sizes:\n",
    "            X[f'wap_lag{shift_size}'] = X.groupby('stock_id')['wap'].shift(shift_size)\n",
    "        return X\n",
    "\n",
    "class LagFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features, shift_sizes):\n",
    "        self.features = features\n",
    "        self.shift_sizes = shift_sizes\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for i in range(len(self.features)):\n",
    "            feature = self.features[i]\n",
    "            shift_sizes = self.shift_sizes[i]\n",
    "            for shift_size in self.shift_sizes:\n",
    "                X[f'{feature}_lag_{shift_size}'] = X.groupby('stock_id')[feature].shift(shift_size)\n",
    "            return X\n",
    "\n",
    "class WapRollingMeanFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window_sizes):\n",
    "        self.window_sizes = window_sizes\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for window_size in self.window_sizes:\n",
    "            X[f'wap_rolling_mean{window_size}'] = X.groupby('stock_id')['wap'].rolling(window=10).mean().reset_index(level=0, drop=True)\n",
    "        return X\n",
    "\n",
    "class WapDiffFeature(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X['wap_diff'] = X.groupby('stock_id')['wap'].diff()\n",
    "        return X\n",
    "\n",
    "class WapExpandingMeanFeature(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X['wap_expanding_mean'] = X.groupby('stock_id')['wap'].expanding().mean().reset_index(level=0, drop=True)\n",
    "        return X\n",
    "\n",
    "class ForwardFillValues(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X.fillna(method='ffill', inplace=True)\n",
    "        return X\n",
    "\n",
    "class FillZero(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.fillna(0)\n",
    "        return X\n",
    "\n",
    "class DataFrameWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, transformer, columns=None):\n",
    "        self.transformer = transformer\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        result = self.transformer.transform(X)\n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            return result\n",
    "        if self.columns is None:\n",
    "            columns = X.columns\n",
    "        else:\n",
    "            columns = []\n",
    "            for column in X.columns:\n",
    "                if column in self.columns:\n",
    "                    columns.append(column)\n",
    "        return pd.DataFrame(result, columns=columns)\n",
    "\n",
    "class PolynomialFeaturesWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2):\n",
    "        self.degree = degree\n",
    "        self.poly = PolynomialFeatures(degree=self.degree, include_bias=False)\n",
    "        self.new_feature_names = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.poly.fit(X)\n",
    "        self.new_feature_names = self.poly.get_feature_names_out(X.columns)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_array = self.poly.transform(X)\n",
    "        return pd.DataFrame(transformed_array, columns=self.new_feature_names)\n",
    "\n",
    "\n",
    "# Column Preprocessor\n",
    "columns_to_keep = [\n",
    "    'imbalance_size_log', 'matched_size_log', 'stock_id', 'imbalance_buy_sell_flag', 'reference_price',\n",
    "    'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap', 'wap_lag_1',\n",
    "    'wap_lag_5', # 'wap_lag10', 'wap_lag15', 'wap_lag20',\n",
    "    'wap_rolling_mean2', 'wap_rolling_mean3',\n",
    "    'wap_rolling_mean4', 'wap_rolling_mean5', 'wap_rolling_mean6', 'wap_rolling_mean10', 'wap_diff',\n",
    "    'wap_expanding_mean', 'seconds_in_bucket', 'date_id',\n",
    "    'matched_size_lag_1', 'matched_size_lag_3', 'matched_size_lag_5'\n",
    "]\n",
    "one_hot_cols = []\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), one_hot_cols),\n",
    "        ('passthrough', 'passthrough', columns_to_keep)\n",
    "    ],\n",
    "    remainder='drop'  # Drop other columns\n",
    ")\n",
    "\n",
    "# Main Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('logs', LogFeatures(['imbalance_size', 'matched_size'])),\n",
    "    ('wap_lags', LagFeatures(['wap'], [1, 5])),\n",
    "    ('matched_size_lags', LagFeatures(['matched_size'], [1, 3, 5])),\n",
    "    ('wap_rolling_means', WapRollingMeanFeatures([2, 3, 4, 5, 6, 10])),\n",
    "    ('wap_diff', WapDiffFeature()),\n",
    "    ('wap_expanding_mean', WapExpandingMeanFeature()),\n",
    "    ('forward_fill', ForwardFillValues()),\n",
    "    ('fill_zero', FillZero()),\n",
    "    ('preprocessor', DataFrameWrapper(preprocessor, columns_to_keep)),\n",
    "    ('scaler', DataFrameWrapper(StandardScaler())),\n",
    "    # ('poly', PolynomialFeaturesWrapper(2)),\n",
    "])\n",
    "\n",
    "train_transformed = pipeline.fit_transform(train)\n",
    "test_transformed = pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_transformed\n",
    "X_test = test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train['wap_lag1'] = train.groupby('stock_id')['wap'].shift(1)\n",
    "# test['wap_lag1'] = test.groupby('stock_id')['wap'].shift(1)\n",
    "\n",
    "# train['wap_lag5'] = train.groupby('stock_id')['wap'].shift(5)\n",
    "# test['wap_lag5'] = test.groupby('stock_id')['wap'].shift(5)\n",
    "\n",
    "# train['wap_rolling_mean10'] = train.groupby('stock_id')['wap'].rolling(window=10).mean().reset_index(level=0, drop=True)\n",
    "# test['wap_rolling_mean10'] = test.groupby('stock_id')['wap'].rolling(window=10).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# train['wap_diff'] = train.groupby('stock_id')['wap'].diff()\n",
    "# test['wap_diff'] = test.groupby('stock_id')['wap'].diff()\n",
    "\n",
    "# train['wap_expanding_mean'] = train.groupby('stock_id')['wap'].expanding().mean().reset_index(level=0, drop=True)\n",
    "# test['wap_expanding_mean'] = test.groupby('stock_id')['wap'].expanding().mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# train = train.fillna(0)\n",
    "# test = train.fillna(0)\n",
    "\n",
    "# train = train.fillna(0)\n",
    "# test = test.fillna(0)\n",
    "\n",
    "# train.fillna(method='ffill', inplace=True)\n",
    "# test.fillna(method='ffill', inplace=True)\n",
    "# train.fillna(train.mean(), inplace=True)\n",
    "\n",
    "# drop row_id column\n",
    "# train = train.drop(columns=['row_id'])\n",
    "# test = test.drop(columns=['row_id'])\n",
    "\n",
    "\n",
    "\n",
    "# X = train.drop(columns='target')\n",
    "# X_test = test.drop(columns='target')\n",
    "\n",
    "# # drop_columns = ['date_id', 'time_id', 'seconds_in_bucket']\n",
    "# drop_columns = ['time_id']\n",
    "# X = X.drop(columns=drop_columns)\n",
    "# X_test = X_test.drop(columns=drop_columns)\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# float_columns = [col for col in X.columns if X[col].dtype == 'float64']\n",
    "# degree2_columns = float_columns + ['stock_id', 'seconds_in_bucket', 'imbalance_buy_sell_flag']\n",
    "# poly_transformer = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('poly2', PolynomialFeatures(degree=(2, 2)), degree2_columns),\n",
    "#     ],\n",
    "#     remainder='passthrough'\n",
    "# )\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('poly_features', poly_transformer)\n",
    "# ])\n",
    "\n",
    "# X = pipeline.fit_transform(X)\n",
    "# X_test = pipeline.transform(X_test)\n",
    "# scale all X columns that are floats\n",
    "# float_columns = [col for col in X.columns if X[col].dtype == 'float64']\n",
    "# scaler = StandardScaler()\n",
    "# X[float_columns] = scaler.fit_transform(X[float_columns])\n",
    "\n",
    "# X['wap_lag1'] = X.groupby('stock_id')['wap'].shift(1)\n",
    "# X_test['wap_lag1'] = X_test.groupby('stock_id')['wap'].shift(1)\n",
    "\n",
    "# X = X.fillna(0)\n",
    "# X_test = X_test.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         -1.700446\n",
       "1         -1.700446\n",
       "2         -1.700446\n",
       "3         -1.700446\n",
       "4         -1.700446\n",
       "             ...   \n",
       "4190379    0.882958\n",
       "4190380    0.945968\n",
       "4190381    0.945968\n",
       "4190382    0.945968\n",
       "4190383    0.945968\n",
       "Name: wap_lag_1, Length: 4190384, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['wap_lag_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_columns = ['date_id', 'time_id']\n",
    "# # drop_columns = []\n",
    "# X = X.drop(columns=drop_columns)\n",
    "# X_test = X_test.drop(columns=drop_columns)\n",
    "# # X = pd.get_dummies(X, columns=['stock_id'], prefix='stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All feature transformations should be combined into a generate_feature() function. This can then be applied to the test data will in the for loop for the Optiver API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(cumulative_test_df, current_test):\n",
    "    cumulative_test_df = pipeline.transform(cumulative_test_df)\n",
    "    # cumulative_test_df['wap_lag1'] = cumulative_test_df.groupby('stock_id')['wap'].shift(1)\n",
    "    # cumulative_test_df['wap_lag5'] = cumulative_test_df.groupby('stock_id')['wap'].shift(5)\n",
    "    # cumulative_test_df['wap_rolling_mean10'] = cumulative_test_df.groupby('stock_id')['wap'].rolling(window=10).mean().reset_index(level=0, drop=True)\n",
    "    # cumulative_test_df['wap_diff'] = cumulative_test_df.groupby('stock_id')['wap'].diff()\n",
    "    # cumulative_test_df['wap_expanding_mean'] = cumulative_test_df.groupby('stock_id')['wap'].expanding().mean().reset_index(level=0, drop=True)\n",
    "    # cumulative_test_df.fillna(method='ffill', inplace=True)\n",
    "    # cumulative_test_df = cumulative_test_df.drop(columns=['row_id'])\n",
    "    # cumulative_test_df = cumulative_test_df.fillna(0)\n",
    "    \n",
    "    # Only return rows corresponding to the current test dataframe\n",
    "    return cumulative_test_df, cumulative_test_df.iloc[-len(current_test):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing\n",
    "\n",
    "Generally, don't change the cell below. It is used to slice the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X\n",
    "y_train = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                  -1.251119e-15\n",
       "date_id                    8.110161e-15\n",
       "seconds_in_bucket         -4.164857e-17\n",
       "imbalance_buy_sell_flag    7.593802e-17\n",
       "reference_price            3.211736e-14\n",
       "far_price                 -1.843614e-15\n",
       "near_price                 1.726448e-14\n",
       "bid_price                 -1.103203e-13\n",
       "bid_size                  -1.552606e-16\n",
       "ask_price                  1.198471e-13\n",
       "ask_size                   1.314791e-16\n",
       "wap                        3.752659e-13\n",
       "imbalance_size_log         1.078970e-13\n",
       "matched_size_log          -4.917885e-14\n",
       "wap_rolling_mean10        -2.936852e-14\n",
       "wap_diff                  -2.936852e-14\n",
       "wap_expanding_mean        -2.936852e-14\n",
       "wap_rolling_mean2         -2.936852e-14\n",
       "wap_rolling_mean3         -2.936852e-14\n",
       "wap_rolling_mean4         -2.936852e-14\n",
       "wap_rolling_mean5         -1.465042e-18\n",
       "wap_rolling_mean6          8.221094e-13\n",
       "wap_lag_1                  4.872622e-17\n",
       "wap_lag_5                  6.333323e-16\n",
       "matched_size_lag_1         2.056689e-16\n",
       "matched_size_lag_3         1.648105e-16\n",
       "matched_size_lag_5        -3.267044e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train type:  <class 'pandas.core.frame.DataFrame'>\n",
      "X_test type:  <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "y_train type:  <class 'numpy.ndarray'>\n",
      "y_test type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# check types of outputs\n",
    "print('X_train type: ', type(X_train))\n",
    "print('X_test type: ', type(X_test))\n",
    "print()\n",
    "print('y_train type: ', type(y_train))\n",
    "print('y_test type: ', type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to numpy arrays\n",
    "# X_train = X_train.values\n",
    "# X_test = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6372\n",
      "[LightGBM] [Info] Number of data points in the train set: 4190384, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "CPU times: user 1min 38s, sys: 3.43 s, total: 1min 42s\n",
      "Wall time: 11.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(n_jobs=-1, objective=&#x27;regression_l1&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(n_jobs=-1, objective=&#x27;regression_l1&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(n_jobs=-1, objective='regression_l1', random_state=0)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# lgbm = lgb.LGBMRegressor(n_jobs=-1, random_state=0, force_col_wise=True,\n",
    "#                          verbose=-1, boosting_type='gbdt', num_leaves=10,\n",
    "#                          reg_alpha=0, reg_lambda=0.2, objective='regression_l1')\n",
    "\n",
    "lgbm = lgb.LGBMRegressor(n_jobs=-1, random_state=0, objective='regression_l1')\n",
    "lgbm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error on the test set: 5.9872\n",
      "Mean Absolute Error on the training set: 6.3652\n"
     ]
    }
   ],
   "source": [
    "# lgbm.score(X_test, y_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = lgbm.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Mean Absolute Error on the test set: {mae:.4f}\")\n",
    "predictions2 = lgbm.predict(X_train)\n",
    "mae2 = mean_absolute_error(y_train, predictions2)\n",
    "print(f\"Mean Absolute Error on the training set: {mae2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.470152 , -16.819836 ,  -2.580285 , ...,   1.1694431,\n",
       "        -1.540184 ,  -6.530285 ])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.1.0\n",
      "CUDA Available:  False\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch Version: ', torch.__version__)\n",
    "print('CUDA Available: ', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('Device Type: ', torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "# X_train = X_train.values\n",
    "# X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(PyTorchMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc3 = nn.Linear(self.hidden_size, hidden_size)  # Output is a single floating point number\n",
    "        self.fc4 = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test, epochs, batch_size, learning_rate, device, verbose=True):\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        criterion = nn.L1Loss()\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "        for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_X = X_train_t[i:i+batch_size]\n",
    "                batch_y = y_train_t[i:i+batch_size]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_X)\n",
    "                loss = criterion(outputs, batch_y.view_as(outputs))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = self(X_test_t)\n",
    "                test_loss = criterion(y_pred, y_test_t.view_as(y_pred))\n",
    "                if verbose:\n",
    "                    print(f'Epoch: {epoch+1}/{epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}')\n",
    "            self.train()\n",
    "\n",
    "    def predict(self, X_test, device):\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_t = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "            y_pred = self(X_test_t)\n",
    "        return y_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model and fitting it on the data from the original sklearn train test split\n",
    "# model = PyTorchMLP(input_size=X_train.shape[1], hidden_size=128)\n",
    "# model.fit(X_train, y_train, X_test, y_test, epochs=2, batch_size=256, learning_rate=0.001, device=device)\n",
    "model = lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions\n",
    "\n",
    "Only for Kaggle:\n",
    "When uploading notebook, make sure to add the Optiver data on the right side of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle_environment:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle_environment:\n",
    "    # To count how many time the \"for loop\" runs.\n",
    "    counter = 0\n",
    "\n",
    "    # init 3 empty lists\n",
    "    test_ls, revealed_targets_ls, sample_prediction_ls = [], [], []\n",
    "    cumulative_test_df = pd.DataFrame()\n",
    "\n",
    "    for (test_in, revealed_targets, sample_prediction) in iter_test:\n",
    "        # Append the dataframe that API return into the list.\n",
    "        test_ls.append(test_in.copy())\n",
    "        revealed_targets_ls.append(revealed_targets.copy())\n",
    "        sample_prediction_ls.append(sample_prediction.copy())\n",
    "\n",
    "        cumulative_test_df = pd.concat([cumulative_test_df, test_in], axis=0, ignore_index=True)\n",
    "\n",
    "        # Generate features\n",
    "        (cumulative_test_df, test_in) = generate_features(cumulative_test_df, test_in)\n",
    "\n",
    "        # Writes our predictions \n",
    "        sample_prediction[\"target\"] = model.predict(test_in)\n",
    "        \n",
    "        # This line submit our predictions.\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "\n",
    "    print('\\n', '=' * 50, sep=\"\")\n",
    "    print(f\"counter: {counter}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
