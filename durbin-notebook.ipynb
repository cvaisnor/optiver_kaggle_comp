{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optiver Trading At The Close\n",
    "\n",
    "[Competition](https://www.kaggle.com/competitions/optiver-trading-at-the-close)\n",
    "\n",
    "[GitHub Repository](https://github.com/cvaisnor/DNN-Final-Project)\n",
    "\n",
    "[Google Slides Presentation](https://docs.google.com/presentation/d/1Xc5F1_NveFi1il3GqHej2aqVmklR_jZU4kx6ZJhiDEM/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "kaggle_environment = False # True if running on Kaggle, don't forget to add the dataset!\n",
    "\n",
    "if kaggle_environment:\n",
    "    data_path = '/kaggle/input/'\n",
    "else:\n",
    "    data_path = 'kaggle/input/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle/input/optiver-trading-at-the-close/.gitkeep\n",
      "kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n",
      "kaggle/input/optiver-trading-at-the-close/train.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/.ipynb_checkpoints/test-checkpoint.csv\n",
      "kaggle/input/optiver-trading-at-the-close/example_test_files/.ipynb_checkpoints/sample_submission-checkpoint.csv\n",
      "kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n",
      "kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk(data_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = pd.read_csv(data_path + 'optiver-trading-at-the-close/train.csv')\n",
    "revealed_targets = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\n",
    "test = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/test.csv')\n",
    "sample_submission = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5237980 entries, 0 to 5237979\n",
      "Data columns (total 17 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   stock_id                 int64  \n",
      " 1   date_id                  int64  \n",
      " 2   seconds_in_bucket        int64  \n",
      " 3   imbalance_size           float64\n",
      " 4   imbalance_buy_sell_flag  int64  \n",
      " 5   reference_price          float64\n",
      " 6   matched_size             float64\n",
      " 7   far_price                float64\n",
      " 8   near_price               float64\n",
      " 9   bid_price                float64\n",
      " 10  bid_size                 float64\n",
      " 11  ask_price                float64\n",
      " 12  ask_size                 float64\n",
      " 13  wap                      float64\n",
      " 14  target                   float64\n",
      " 15  time_id                  int64  \n",
      " 16  row_id                   object \n",
      "dtypes: float64(11), int64(5), object(1)\n",
      "memory usage: 679.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33000 entries, 0 to 32999\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   stock_id                 33000 non-null  int64  \n",
      " 1   date_id                  33000 non-null  int64  \n",
      " 2   seconds_in_bucket        33000 non-null  int64  \n",
      " 3   imbalance_size           33000 non-null  float64\n",
      " 4   imbalance_buy_sell_flag  33000 non-null  int64  \n",
      " 5   reference_price          33000 non-null  float64\n",
      " 6   matched_size             33000 non-null  float64\n",
      " 7   far_price                14771 non-null  float64\n",
      " 8   near_price               15000 non-null  float64\n",
      " 9   bid_price                33000 non-null  float64\n",
      " 10  bid_size                 33000 non-null  float64\n",
      " 11  ask_price                33000 non-null  float64\n",
      " 12  ask_size                 33000 non-null  float64\n",
      " 13  wap                      33000 non-null  float64\n",
      " 14  time_id                  33000 non-null  int64  \n",
      " 15  row_id                   33000 non-null  object \n",
      "dtypes: float64(10), int64(5), object(1)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "original_train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "split_idx = int(len(original_train) * split_ratio)\n",
    "\n",
    "train = original_train.iloc[:split_idx]\n",
    "test = original_train.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "<timed exec>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<timed exec>:23: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "<timed exec>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.4 s, sys: 1.56 s, total: 5.96 s\n",
      "Wall time: 5.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['wap_lag1'] = train.groupby('stock_id')['wap'].shift(1)\n",
    "test['wap_lag1'] = test.groupby('stock_id')['wap'].shift(1)\n",
    "\n",
    "train['wap_lag5'] = train.groupby('stock_id')['wap'].shift(5)\n",
    "test['wap_lag5'] = test.groupby('stock_id')['wap'].shift(5)\n",
    "\n",
    "train['wap_rolling_mean10'] = train.groupby('stock_id')['wap'].rolling(window=10).mean().reset_index(level=0, drop=True)\n",
    "test['wap_rolling_mean10'] = test.groupby('stock_id')['wap'].rolling(window=10).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "train['wap_diff'] = train.groupby('stock_id')['wap'].diff()\n",
    "test['wap_diff'] = test.groupby('stock_id')['wap'].diff()\n",
    "\n",
    "train['wap_expanding_mean'] = train.groupby('stock_id')['wap'].expanding().mean().reset_index(level=0, drop=True)\n",
    "test['wap_expanding_mean'] = test.groupby('stock_id')['wap'].expanding().mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# train = train.fillna(0)\n",
    "# test = train.fillna(0)\n",
    "\n",
    "# train = train.fillna(0)\n",
    "# test = test.fillna(0)\n",
    "\n",
    "train.fillna(method='ffill', inplace=True)\n",
    "test.fillna(method='ffill', inplace=True)\n",
    "# train.fillna(train.mean(), inplace=True)\n",
    "\n",
    "# drop row_id column\n",
    "train = train.drop(columns=['row_id'])\n",
    "test = test.drop(columns=['row_id'])\n",
    "\n",
    "y = train['target'].values\n",
    "y_test = test['target'].values\n",
    "\n",
    "X = train.drop(columns='target')\n",
    "X_test = test.drop(columns='target')\n",
    "\n",
    "# drop_columns = ['date_id', 'time_id', 'seconds_in_bucket']\n",
    "drop_columns = ['time_id']\n",
    "X = X.drop(columns=drop_columns)\n",
    "X_test = X_test.drop(columns=drop_columns)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# float_columns = [col for col in X.columns if X[col].dtype == 'float64']\n",
    "# degree2_columns = float_columns + ['stock_id', 'seconds_in_bucket', 'imbalance_buy_sell_flag']\n",
    "# poly_transformer = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('poly2', PolynomialFeatures(degree=(2, 2)), degree2_columns),\n",
    "#     ],\n",
    "#     remainder='passthrough'\n",
    "# )\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('poly_features', poly_transformer)\n",
    "# ])\n",
    "\n",
    "# X = pipeline.fit_transform(X)\n",
    "# X_test = pipeline.transform(X_test)\n",
    "# scale all X columns that are floats\n",
    "# float_columns = [col for col in X.columns if X[col].dtype == 'float64']\n",
    "# scaler = StandardScaler()\n",
    "# X[float_columns] = scaler.fit_transform(X[float_columns])\n",
    "\n",
    "# X['wap_lag1'] = X.groupby('stock_id')['wap'].shift(1)\n",
    "# X_test['wap_lag1'] = X_test.groupby('stock_id')['wap'].shift(1)\n",
    "\n",
    "X = X.fillna(0)\n",
    "X_test = X_test.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.000000\n",
       "1          0.000000\n",
       "2          0.000000\n",
       "3          0.000000\n",
       "4          0.000000\n",
       "             ...   \n",
       "4190379    0.998839\n",
       "4190380    1.000783\n",
       "4190381    1.002925\n",
       "4190382    1.002209\n",
       "4190383    0.999564\n",
       "Name: wap_lag1, Length: 4190384, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['wap_lag1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_columns = ['date_id', 'time_id']\n",
    "# # drop_columns = []\n",
    "# X = X.drop(columns=drop_columns)\n",
    "# X_test = X_test.drop(columns=drop_columns)\n",
    "# # X = pd.get_dummies(X, columns=['stock_id'], prefix='stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All feature transformations should be combined into a generate_feature() function. This can then be applied to the test data will in the for loop for the Optiver API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(cumulative_test_df, current_test):\n",
    "    cumulative_test_df['wap_lag1'] = cumulative_test_df.groupby('stock_id')['wap'].shift(1)\n",
    "    cumulative_test_df['wap_lag5'] = cumulative_test_df.groupby('stock_id')['wap'].shift(5)\n",
    "    cumulative_test_df['wap_rolling_mean10'] = cumulative_test_df.groupby('stock_id')['wap'].rolling(window=10).mean().reset_index(level=0, drop=True)\n",
    "    cumulative_test_df['wap_diff'] = cumulative_test_df.groupby('stock_id')['wap'].diff()\n",
    "    cumulative_test_df['wap_expanding_mean'] = cumulative_test_df.groupby('stock_id')['wap'].expanding().mean().reset_index(level=0, drop=True)\n",
    "    cumulative_test_df.fillna(method='ffill', inplace=True)\n",
    "    cumulative_test_df = cumulative_test_df.drop(columns=['row_id'])\n",
    "    cumulative_test_df = cumulative_test_df.fillna(0)\n",
    "    \n",
    "    # Only return rows corresponding to the current test dataframe\n",
    "    return cumulative_test_df, cumulative_test_df.iloc[-len(current_test):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing\n",
    "\n",
    "Generally, don't change the cell below. It is used to slice the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X\n",
    "y_train = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                   9.923560e+01\n",
       "date_id                    1.936673e+02\n",
       "seconds_in_bucket          2.698698e+02\n",
       "imbalance_size             5.619934e+06\n",
       "imbalance_buy_sell_flag    8.151998e-04\n",
       "reference_price            1.000002e+00\n",
       "matched_size               4.460104e+07\n",
       "far_price                  9.997329e-01\n",
       "near_price                 9.985859e-01\n",
       "bid_price                  9.997305e-01\n",
       "bid_size                   5.033497e+04\n",
       "ask_price                  1.000271e+00\n",
       "ask_size                   5.189606e+04\n",
       "wap                        9.999981e-01\n",
       "wap_lag1                   9.999524e-01\n",
       "wap_lag5                   9.997699e-01\n",
       "wap_rolling_mean10         9.995875e-01\n",
       "wap_diff                   4.745914e-08\n",
       "wap_expanding_mean         1.000009e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train type:  <class 'pandas.core.frame.DataFrame'>\n",
      "X_test type:  <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "y_train type:  <class 'numpy.ndarray'>\n",
      "y_test type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# check types of outputs\n",
    "print('X_train type: ', type(X_train))\n",
    "print('X_test type: ', type(X_test))\n",
    "print()\n",
    "print('y_train type: ', type(y_train))\n",
    "print('y_test type: ', type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to numpy arrays\n",
    "# X_train = X_train.values\n",
    "# X_test = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4326\n",
      "[LightGBM] [Info] Number of data points in the train set: 4190384, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "CPU times: user 1min 30s, sys: 5.92 s, total: 1min 36s\n",
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(n_jobs=-1, objective=&#x27;regression_l1&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(n_jobs=-1, objective=&#x27;regression_l1&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(n_jobs=-1, objective='regression_l1', random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# lgbm = lgb.LGBMRegressor(n_jobs=-1, random_state=0, force_col_wise=True,\n",
    "#                          verbose=-1, boosting_type='gbdt', num_leaves=10,\n",
    "#                          reg_alpha=0, reg_lambda=0.2, objective='regression_l1')\n",
    "\n",
    "lgbm = lgb.LGBMRegressor(n_jobs=-1, random_state=0, objective='regression_l1')\n",
    "\n",
    "# m = Pipeline([\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"model\", lgbm)\n",
    "# ])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "lgbm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error on the test set: 5.9880\n",
      "Mean Absolute Error on the training set: 6.3663\n"
     ]
    }
   ],
   "source": [
    "# lgbm.score(X_test, y_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = lgbm.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Mean Absolute Error on the test set: {mae:.4f}\")\n",
    "predictions2 = lgbm.predict(X_train)\n",
    "mae2 = mean_absolute_error(y_train, predictions2)\n",
    "print(f\"Mean Absolute Error on the training set: {mae2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.470152 , -16.819836 ,  -2.580285 , ...,   1.1694431,\n",
       "        -1.540184 ,  -6.530285 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.1.0\n",
      "CUDA Available:  False\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch Version: ', torch.__version__)\n",
    "print('CUDA Available: ', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('Device Type: ', torch.cuda.get_device_name(0))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "# X_train = X_train.values\n",
    "# X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(PyTorchMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc3 = nn.Linear(self.hidden_size, hidden_size)  # Output is a single floating point number\n",
    "        self.fc4 = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test, epochs, batch_size, learning_rate, device, verbose=True):\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        criterion = nn.L1Loss()\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "        for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_X = X_train_t[i:i+batch_size]\n",
    "                batch_y = y_train_t[i:i+batch_size]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_X)\n",
    "                loss = criterion(outputs, batch_y.view_as(outputs))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = self(X_test_t)\n",
    "                test_loss = criterion(y_pred, y_test_t.view_as(y_pred))\n",
    "                if verbose:\n",
    "                    print(f'Epoch: {epoch+1}/{epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}')\n",
    "            self.train()\n",
    "\n",
    "    def predict(self, X_test, device):\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_t = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "            y_pred = self(X_test_t)\n",
    "        return y_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model and fitting it on the data from the original sklearn train test split\n",
    "# model = PyTorchMLP(input_size=X_train.shape[1], hidden_size=128)\n",
    "# model.fit(X_train, y_train, X_test, y_test, epochs=2, batch_size=256, learning_rate=0.001, device=device)\n",
    "model = lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions\n",
    "\n",
    "Only for Kaggle:\n",
    "When uploading notebook, make sure to add the Optiver data on the right side of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle_environment:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle_environment:\n",
    "    # To count how many time the \"for loop\" runs.\n",
    "    counter = 0\n",
    "\n",
    "    # init 3 empty lists\n",
    "    test_ls, revealed_targets_ls, sample_prediction_ls = [], [], []\n",
    "    cumulative_test_df = pd.DataFrame()\n",
    "\n",
    "    for (test_in, revealed_targets, sample_prediction) in iter_test:\n",
    "        # Append the dataframe that API return into the list.\n",
    "        test_ls.append(test_in.copy())\n",
    "        revealed_targets_ls.append(revealed_targets.copy())\n",
    "        sample_prediction_ls.append(sample_prediction.copy())\n",
    "\n",
    "        cumulative_test_df = pd.concat([cumulative_test_df, test_in], axis=0, ignore_index=True)\n",
    "\n",
    "        # Generate features\n",
    "        (cumulative_test_df, test_in) = generate_features(cumulative_test_df, test_in)\n",
    "\n",
    "        # Writes our predictions \n",
    "        sample_prediction[\"target\"] = model.predict(test_in)\n",
    "        \n",
    "        # This line submit our predictions.\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "\n",
    "    print('\\n', '=' * 50, sep=\"\")\n",
    "    print(f\"counter: {counter}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
