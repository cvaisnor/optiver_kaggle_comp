{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Optiver Trading At The Close\n","\n","[Competition](https://www.kaggle.com/competitions/optiver-trading-at-the-close)\n","\n","[GitHub Repository](https://github.com/cvaisnor/DNN-Final-Project)\n","\n","[Google Slides Presentation](https://docs.google.com/presentation/d/1Xc5F1_NveFi1il3GqHej2aqVmklR_jZU4kx6ZJhiDEM/edit?usp=sharing)"]},{"cell_type":"markdown","metadata":{},"source":["# General Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.443451Z","iopub.status.busy":"2023-10-17T21:29:46.442668Z","iopub.status.idle":"2023-10-17T21:29:46.826370Z","shell.execute_reply":"2023-10-17T21:29:46.825504Z","shell.execute_reply.started":"2023-10-17T21:29:46.443417Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","kaggle_environment = False # True if running on Kaggle, don't forget to add the dataset!\n","\n","if kaggle_environment:\n","    data_path = '/kaggle/input/'\n","else:\n","    data_path = 'kaggle/input/'\n","\n","# disable PkgResourcesDeprecationWarning\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Setting File Structure"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.829068Z","iopub.status.busy":"2023-10-17T21:29:46.828481Z","iopub.status.idle":"2023-10-17T21:29:46.840764Z","shell.execute_reply":"2023-10-17T21:29:46.839513Z","shell.execute_reply.started":"2023-10-17T21:29:46.829013Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n","kaggle/input/optiver-trading-at-the-close/.gitkeep\n","kaggle/input/optiver-trading-at-the-close/train.csv\n","kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n","kaggle/input/optiver-trading-at-the-close/optiver2023/__pycache__/__init__.cpython-310.pyc\n","kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n","kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n","kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n"]}],"source":["for dirname, _, filenames in os.walk(data_path):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["# Loading Data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.843600Z","iopub.status.busy":"2023-10-17T21:29:46.842562Z","iopub.status.idle":"2023-10-17T21:30:06.373150Z","shell.execute_reply":"2023-10-17T21:30:06.371968Z","shell.execute_reply.started":"2023-10-17T21:29:46.843565Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(data_path + 'optiver-trading-at-the-close/train.csv')\n","revealed_targets = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\n","test = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/test.csv')\n","sample_submission = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/sample_submission.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Data Descriptions\n","\n","- stock_id - A unique identifier for the stock. Not all stock IDs exist in every time bucket.\n","- date_id - A unique identifier for the date. Date IDs are sequential & consistent across all stocks.\n","- imbalance_size - The amount unmatched at the current reference price (in USD).\n","- imbalance_buy_sell_flag - An indicator reflecting the direction of auction imbalance.\n","    - buy-side imbalance; 1\n","    - sell-side imbalance; -1\n","    - no imbalance; 0\n","\n","- reference_price - The price at which paired shares are maximized, the imbalance is minimized and the distance from the bid-ask midpoint is minimized, in that order. Can also be thought of as being equal to the near price bounded between the best bid and ask price.\n","- matched_size - The amount that can be matched at the current reference price (in USD).\n","- far_price - The crossing price that will maximize the number of shares matched based on auction interest only. This calculation excludes continuous market orders.\n","- near_price - The crossing price that will maximize the number of shares matched based auction and continuous market orders.\n","- [bid/ask]_price - Price of the most competitive buy/sell level in the non-auction book.\n","- [bid/ask]_size - The dollar notional amount on the most competitive buy/sell level in the non-auction book.\n","- wap - The weighted average price in the non-auction book.\n","- seconds_in_bucket - The number of seconds elapsed since the beginning of the day's closing auction, always starting from 0.\n","- target - The 60 second future move in the wap of the stock, less the 60 second future move of the synthetic index. Only provided for the train set.\n","    - The synthetic index is a custom weighted index of Nasdaq-listed stocks constructed by Optiver for this competition.\n","    - The unit of the target is basis points, which is a common unit of measurement in financial markets. A 1 basis point price move is equivalent to a 0.01% price move.\n","    - Where t is the time at the current observation, we can define the target:\n","                   \n","---\n","\n","All size related columns are in USD terms.\n","\n","All price related columns are converted to a price move relative to the stock wap (weighted average price) at the beginning of the auction period.\n","\n","sample_submission A valid sample submission, delivered by the API. See this notebook for a very simple example of how to use the sample submission.\n","\n","revealed_targets When the first time_id for each date (i.e. when seconds_in_bucket equals zero) the API will serve a dataframe providing the true target values for the entire previous date. All other rows contain null values for the columns of interest.\n","\n","public_timeseries_testing_util.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it.\n","\n","example_test_files/ Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three date ids are repeats of the last three date ids in the train set, to enable an illustration of how the API functions.\n","\n","optiver2023/ Files that enable the API. Expect the API to deliver all rows in under five minutes and to reserve less than 0.5 GB of memory. The first three date ids delivered by the API are repeats of the last three date ids in the train set, to better illustrate how the API functions. You must make predictions for those dates in order to advance the API but those predictions are not scored."]},{"cell_type":"markdown","metadata":{},"source":["# Feature Engineering Section"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","\n","class CustomIterativeImputer(BaseEstimator, TransformerMixin):\n","    def __init__(self, columns, max_iter=10, random_state=0):\n","        self.columns = columns\n","        self.max_iter = max_iter\n","        self.random_state = random_state\n","        self.imputer = None\n","\n","    def fit(self, X, y=None):\n","        # Initialize the IterativeImputer\n","        self.imputer = IterativeImputer(max_iter=self.max_iter, random_state=self.random_state)\n","        self.imputer.fit(X[self.columns])\n","        return self\n","\n","    def transform(self, X):\n","        # Check is fit had been called\n","        if self.imputer is None:\n","            raise RuntimeError(\"You must call fit before calling transform\")\n","        \n","        # Fit the imputer and transform the data\n","        X[self.columns] = self.imputer.transform(X[self.columns])\n","        return X\n","\n","# Create the pipeline with the custom imputer\n","pipeline = Pipeline([\n","    ('iterative_imputer', CustomIterativeImputer(columns=['far_price', 'near_price'])),\n","    # Add other steps here if necessary\n","])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Use the pipeline on the training data\n","df_processed = pipeline.fit_transform(train)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# drop all rows with NaNs (500-ish rows after imputation)\n","df_processed = df_processed.dropna()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def add_datetime_feature(df):\n","    \"\"\"\n","    Add datetime features to a DataFrame.\n","\n","    Parameters:\n","    df (pandas.DataFrame): DataFrame to process.\n","\n","    Returns:\n","    pandas.DataFrame: DataFrame with added datetime feature.\n","    \"\"\"\n","    df = df.copy()\n","    start_time = pd.to_timedelta('9:30:00')\n","    # Convert date_id to actual dates starting from 2020-01-01\n","    df['date'] = pd.to_datetime('2020-01-01') + pd.to_timedelta(df['date_id'], unit='D')\n","    df['time'] = pd.to_timedelta(df['time_id'], unit='s')\n","    # Add the trading start time to the time column\n","    df['time'] = df['time'] + start_time\n","    # Create long-form datetime timestamp\n","    df['timestamp'] = df['date'] + df['time']\n","    df.drop(['date', 'time', 'time_id', 'date_id'], axis=1, inplace=True)\n","    return df"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["df_with_datetime = add_datetime_feature(df_processed)"]},{"cell_type":"markdown","metadata":{},"source":["Comprehensive function for all feature engineering"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:06.382936Z","iopub.status.busy":"2023-10-17T21:30:06.382593Z","iopub.status.idle":"2023-10-17T21:30:06.396015Z","shell.execute_reply":"2023-10-17T21:30:06.394500Z","shell.execute_reply.started":"2023-10-17T21:30:06.382907Z"},"trusted":true},"outputs":[],"source":["def generate_features(test):\n","    # test is a dataframe with the same columns as train with only 200 rows\n","    test = test.copy()\n","\n","    # Add datetime features\n","    test = add_datetime_feature(test)\n","\n","    return test"]},{"cell_type":"markdown","metadata":{},"source":["# Model Development"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.613776Z","iopub.status.busy":"2023-10-17T21:30:10.613395Z","iopub.status.idle":"2023-10-17T21:30:10.641355Z","shell.execute_reply":"2023-10-17T21:30:10.640398Z","shell.execute_reply.started":"2023-10-17T21:30:10.613732Z"},"trusted":true},"outputs":[],"source":["if kaggle_environment:\n","    import optiver2023\n","    env = optiver2023.make_env()\n","    iter_test = env.iter_test()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import copy\n","from pathlib import Path\n","import warnings\n","\n","import lightning.pytorch as pl\n","from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n","from lightning.pytorch.loggers import TensorBoardLogger\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n","from pytorch_forecasting.data import GroupNormalizer\n","from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n","from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pytorch_forecasting.data.examples import get_stallion_data\n","\n","data = get_stallion_data()\n","\n","# add time index\n","data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n","data[\"time_idx\"] -= data[\"time_idx\"].min()\n","\n","# add additional features\n","data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n","data[\"log_volume\"] = np.log(data.volume + 1e-8)\n","data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n","data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_prediction_length = 6\n","max_encoder_length = 24\n","training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n","\n","training = TimeSeriesDataSet(\n","    data[lambda x: x.time_idx <= training_cutoff],\n","    time_idx=\"time_idx\",\n","    target=\"volume\",\n","    group_ids=[\"agency\", \"sku\"],\n","    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n","    max_encoder_length=max_encoder_length,\n","    min_prediction_length=1,\n","    max_prediction_length=max_prediction_length,\n","    static_categoricals=[\"agency\", \"sku\"],\n","    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n","    time_varying_known_categoricals=[\"month\"],\n","    # variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n","    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n","    time_varying_unknown_categoricals=[],\n","    time_varying_unknown_reals=[\n","        \"volume\",\n","        \"log_volume\",\n","        \"industry_volume\",\n","        \"soda_volume\",\n","        \"avg_max_temp\",\n","        \"avg_volume_by_agency\",\n","        \"avg_volume_by_sku\",\n","    ],\n","    target_normalizer=GroupNormalizer(\n","        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n","    ),  # use softplus and normalize by group\n","    add_relative_time_idx=True,\n","    add_target_scales=True,\n","    add_encoder_length=True,\n",")\n","\n","# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n","# for each series\n","validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n","\n","# create dataloaders for model\n","batch_size = 128  # set this between 32 to 128\n","train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n","val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n","baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n","MAE()(baseline_predictions.output, baseline_predictions.y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# configure network and trainer\n","pl.seed_everything(42)\n","trainer = pl.Trainer(\n","    accelerator=\"gpu\",\n","    # clipping gradients is a hyperparameter and important to prevent divergance\n","    # of the gradient for recurrent neural networks\n","    gradient_clip_val=0.1,\n",")\n","\n","\n","tft = TemporalFusionTransformer.from_dataset(\n","    training,\n","    # not meaningful for finding the learning rate but otherwise very important\n","    learning_rate=0.03,\n","    hidden_size=8,  # most important hyperparameter apart from learning rate\n","    # number of attention heads. Set to up to 4 for large datasets\n","    attention_head_size=1,\n","    dropout=0.1,  # between 0.1 and 0.3 are good values\n","    hidden_continuous_size=8,  # set to <= hidden_size\n","    loss=QuantileLoss(),\n","    optimizer=\"Ranger\"\n","    # reduce learning rate if no improvement in validation loss after x epochs\n","    # reduce_on_plateau_patience=1000,\n",")\n","print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# find optimal learning rate\n","from lightning.pytorch.tuner import Tuner\n","\n","res = Tuner(trainer).lr_find(\n","    tft,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n","    max_lr=10.0,\n","    min_lr=1e-6,\n",")\n","\n","print(f\"suggested learning rate: {res.suggestion()}\")\n","fig = res.plot(show=True, suggest=True)\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n","lr_logger = LearningRateMonitor()  # log the learning rate\n","# logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n","\n","trainer = pl.Trainer(\n","    max_epochs=50,\n","    accelerator=\"gpu\",\n","    enable_model_summary=True,\n","    gradient_clip_val=0.1,\n","    limit_train_batches=50,  # comment in for training, running valiation every 30 batches\n","    fast_dev_run=True,  # comment in to check that network dataset has no serious bugs\n","    callbacks=[lr_logger, early_stop_callback],\n","    # logger=logger,\n",")\n","\n","tft = TemporalFusionTransformer.from_dataset(\n","    training,\n","    learning_rate=0.03,\n","    hidden_size=16,\n","    attention_head_size=2,\n","    dropout=0.1,\n","    hidden_continuous_size=8,\n","    loss=QuantileLoss(),\n","    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n","    optimizer=\"Ranger\",\n","    reduce_on_plateau_patience=4,\n",")\n","print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# fit network\n","trainer.fit(\n","    tft,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.643482Z","iopub.status.busy":"2023-10-17T21:30:10.642850Z","iopub.status.idle":"2023-10-17T21:30:11.480530Z","shell.execute_reply":"2023-10-17T21:30:11.479410Z","shell.execute_reply.started":"2023-10-17T21:30:10.643452Z"},"trusted":true},"outputs":[],"source":["if kaggle_environment:\n","    # To count how many time the \"for loop\" runs.\n","    counter = 0\n","\n","    # init 3 empty lists\n","    test_ls, revealed_targets_ls, sample_prediction_ls = [], [], []\n","\n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","        # Append the dataframe that API return into the list.\n","        test_ls.append(test.copy())\n","        revealed_targets_ls.append(revealed_targets.copy())\n","        sample_prediction_ls.append(sample_prediction.copy())\n","\n","        # Generate features\n","        test = generate_features(test)\n","\n","        # Writes our predictions \n","        sample_prediction[\"target\"] = predictor.predict(test)\n","        \n","        # This line submit our predictions.\n","        env.predict(sample_prediction)\n","        counter += 1\n","\n","    print('\\n', '=' * 50, sep=\"\")\n","    print(f\"counter: {counter}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
