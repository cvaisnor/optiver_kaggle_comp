{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Optiver Trading At The Close\n","\n","[Competition](https://www.kaggle.com/competitions/optiver-trading-at-the-close)\n","\n","[GitHub Repository](https://github.com/cvaisnor/DNN-Final-Project)\n","\n","[Google Slides Presentation](https://docs.google.com/presentation/d/1Xc5F1_NveFi1il3GqHej2aqVmklR_jZU4kx6ZJhiDEM/edit?usp=sharing)"]},{"cell_type":"markdown","metadata":{},"source":["# General Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.443451Z","iopub.status.busy":"2023-10-17T21:29:46.442668Z","iopub.status.idle":"2023-10-17T21:29:46.826370Z","shell.execute_reply":"2023-10-17T21:29:46.825504Z","shell.execute_reply.started":"2023-10-17T21:29:46.443417Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","kaggle_environment = False # True if running on Kaggle, don't forget to add the dataset!\n","\n","if kaggle_environment:\n","    data_path = '/kaggle/input/'\n","else:\n","    data_path = 'kaggle/input/'\n","\n","# disable PkgResourcesDeprecationWarning\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Setting File Structure"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.829068Z","iopub.status.busy":"2023-10-17T21:29:46.828481Z","iopub.status.idle":"2023-10-17T21:29:46.840764Z","shell.execute_reply":"2023-10-17T21:29:46.839513Z","shell.execute_reply.started":"2023-10-17T21:29:46.829013Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n","kaggle/input/optiver-trading-at-the-close/.gitkeep\n","kaggle/input/optiver-trading-at-the-close/train.csv\n","kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n","kaggle/input/optiver-trading-at-the-close/optiver2023/__pycache__/__init__.cpython-310.pyc\n","kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n","kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n","kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n"]}],"source":["for dirname, _, filenames in os.walk(data_path):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["# Loading Data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.843600Z","iopub.status.busy":"2023-10-17T21:29:46.842562Z","iopub.status.idle":"2023-10-17T21:30:06.373150Z","shell.execute_reply":"2023-10-17T21:30:06.371968Z","shell.execute_reply.started":"2023-10-17T21:29:46.843565Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(data_path + 'optiver-trading-at-the-close/train.csv')\n","revealed_targets = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\n","test = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/test.csv')\n","sample_submission = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/sample_submission.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Engineering Section"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","\n","class CustomIterativeImputer(BaseEstimator, TransformerMixin):\n","    def __init__(self, columns, max_iter=10, random_state=0):\n","        self.columns = columns\n","        self.max_iter = max_iter\n","        self.random_state = random_state\n","        self.imputer = None\n","\n","    def fit(self, X, y=None):\n","        # Initialize the IterativeImputer\n","        self.imputer = IterativeImputer(max_iter=self.max_iter, random_state=self.random_state)\n","        self.imputer.fit(X[self.columns])\n","        return self\n","\n","    def transform(self, X):\n","        # Check is fit had been called\n","        if self.imputer is None:\n","            raise RuntimeError(\"You must call fit before calling transform\")\n","        \n","        # Fit the imputer and transform the data\n","        X[self.columns] = self.imputer.transform(X[self.columns])\n","        return X\n","\n","# Create the pipeline with the custom imputer\n","pipeline = Pipeline([\n","    ('iterative_imputer', CustomIterativeImputer(columns=['far_price', 'near_price'])),\n","    # Add other steps here if necessary\n","])"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Use the pipeline on the training data\n","df_processed = pipeline.fit_transform(train)\n","\n","# drop all rows with NaNs (500-ish rows after imputation)\n","df_processed = df_processed.dropna()\n","\n","# drop row_id column\n","df_processed = df_processed.drop(columns=['row_id'])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# def add_datetime_feature(df):\n","#     \"\"\"\n","#     Add datetime features to a DataFrame.\n","\n","#     Parameters:\n","#     df (pandas.DataFrame): DataFrame to process.\n","\n","#     Returns:\n","#     pandas.DataFrame: DataFrame with added datetime feature.\n","#     \"\"\"\n","#     df = df.copy()\n","#     start_time = pd.to_timedelta('9:30:00')\n","#     # Convert date_id to actual dates starting from 2020-01-01\n","#     df['date'] = pd.to_datetime('2020-01-01') + pd.to_timedelta(df['date_id'], unit='D')\n","#     df['time'] = pd.to_timedelta(df['time_id'], unit='s')\n","#     # Add the trading start time to the time column\n","#     df['time'] = df['time'] + start_time\n","#     # Create long-form datetime timestamp\n","#     df['timestamp'] = df['date'] + df['time']\n","#     df.drop(['date', 'time', 'date_id'], axis=1, inplace=True)\n","\n","#     # add day\n","#     df['day'] = df['timestamp'].dt.day\n","\n","#     # Add unix time\n","#     df['unix_time'] = df['timestamp'].astype(np.int64) // 10**9\n","\n","#     # Add a time_idx (an sequence of consecutive integers that goes from min to max date)\n","#     df['time_idx'] = df['timestamp'].rank(method='dense').astype(int) - 1\n","\n","#     return df"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# df_with_datetime = add_datetime_feature(df_processed)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def generate_features(test):\n","    return test"]},{"cell_type":"markdown","metadata":{},"source":["# Model Development"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import os\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-06 14:27:40.891533: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-12-06 14:27:40.909672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import copy\n","from pathlib import Path\n","import warnings\n","\n","import lightning.pytorch as pl\n","from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n","from lightning.pytorch.loggers import TensorBoardLogger\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n","from pytorch_forecasting.data import GroupNormalizer\n","from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n","from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using torch 2.1.1+cu121\n","Using pytorch lightning 2.1.2\n"]}],"source":["# print pytorch version\n","print(f\"Using torch {torch.__version__}\")\n","\n","# print pytorch lightning version\n","print(f\"Using pytorch lightning {pl.__version__}\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# we want to predict 1 minute ahead of each stock_id\n","max_prediction_length = 60 # (seconds)\n","\n","# max encoder length = number of seconds in 7 hours\n","max_encoder_length = 7 * 60 * 60 # (seconds)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# convert \"stock_id\" to string so it is categorical\n","df_processed['stock_id'] = df_processed['stock_id'].astype(str)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# creating the dataset\n","training = TimeSeriesDataSet(\n","    df_processed,\n","    time_idx=\"time_id\",\n","    target=\"target\",\n","    group_ids=[\"stock_id\"],\n","    min_encoder_length=0,  # allow predictions without history\n","    max_encoder_length=max_encoder_length,\n","    min_prediction_length=1,\n","    max_prediction_length=max_prediction_length,\n","    static_categoricals=[\"stock_id\"],\n","    time_varying_unknown_reals=[\"target\"],\n","    allow_missing_timesteps=True,\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# TypeError: Addition/subtraction of integers and integer-arrays with Timestamp is no longer supported.  Instead of adding/subtracting `n`, use `n * obj.freq`\n","\n","# create validation set (predict=True) which means to predict the last max_prediction_length points in time for each series\n","validation = TimeSeriesDataSet.from_dataset(training, df_processed, predict=True, stop_randomization=True)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# creating dataloaders\n","batch_size = 128  # set this between 32 to 128\n","train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n","val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"text/plain":["tensor(6.0678, device='cuda:0')"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n","baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n","MAE()(baseline_predictions.output, baseline_predictions.y)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Seed set to 42\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["Number of parameters in network: 6.2k\n"]}],"source":["PATIENCE = 30\n","MAX_EPOCHS = 120\n","LEARNING_RATE = 0.03\n","OPTUNA = False\n","\n","# configure network and trainer\n","pl.seed_everything(42)\n","trainer = pl.Trainer(\n","    accelerator=\"gpu\",\n","    # clipping gradients is a hyperparameter and important to prevent divergance\n","    # of the gradient for recurrent neural networks\n","    gradient_clip_val=0.1,\n",")\n","\n","tft = TemporalFusionTransformer.from_dataset(\n","    training,\n","    # not meaningful for finding the learning rate but otherwise very important\n","    learning_rate=0.03,\n","    hidden_size=8,  # most important hyperparameter apart from learning rate\n","    # number of attention heads. Set to up to 4 for large datasets\n","    attention_head_size=1,\n","    dropout=0.1,  # between 0.1 and 0.3 are good values\n","    hidden_continuous_size=8,  # set to <= hidden_size\n","    loss=QuantileLoss(),\n","    optimizer=\"Ranger\",\n","    # reduce learning rate if no improvement in validation loss after x epochs\n","    reduce_on_plateau_patience=1000,\n",")\n","print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31b209cc38db42cea6f39173c4247224","version_major":2,"version_minor":0},"text/plain":["Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"AttributeError","evalue":"'LearningRateFinder' object has no attribute 'optimal_lr'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24328/4268711780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m res = Tuner(trainer).lr_find(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/tuner/tuning.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlr_finder_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlr_finder_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimal_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'LearningRateFinder' object has no attribute 'optimal_lr'"]}],"source":["# find optimal learning rate\n","from lightning.pytorch.tuner import Tuner\n","\n","res = Tuner(trainer).lr_find(\n","    tft,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n","    max_lr=10.0,\n","    min_lr=1e-6,\n",")\n","\n","print(f\"suggested learning rate: {res.suggestion()}\")\n","fig = res.plot(show=True, suggest=True)\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tft = TemporalFusionTransformer.from_dataset(\n","    training,\n","    learning_rate=LEARNING_RATE,\n","    hidden_size=16,  # biggest influence network size\n","    attention_head_size=1,\n","    dropout=0.1,\n","    hidden_continuous_size=8,\n","    output_size=7,  # QuantileLoss has 7 quantiles by default\n","    loss=QuantileLoss(),\n","    # reduce learning rate if no improvement in validation loss after x epochs\n","    reduce_on_plateau_patience=4,\n",")\n","\n","print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# fit network\n","trainer.fit(\n","    tft,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Generating Predictions\n","\n","Only for Kaggle:\n","When uploading notebook, make sure to add the Optiver data on the right side of the screen."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.613776Z","iopub.status.busy":"2023-10-17T21:30:10.613395Z","iopub.status.idle":"2023-10-17T21:30:10.641355Z","shell.execute_reply":"2023-10-17T21:30:10.640398Z","shell.execute_reply.started":"2023-10-17T21:30:10.613732Z"},"trusted":true},"outputs":[],"source":["if kaggle_environment:\n","    import optiver2023\n","    env = optiver2023.make_env()\n","    iter_test = env.iter_test()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.643482Z","iopub.status.busy":"2023-10-17T21:30:10.642850Z","iopub.status.idle":"2023-10-17T21:30:11.480530Z","shell.execute_reply":"2023-10-17T21:30:11.479410Z","shell.execute_reply.started":"2023-10-17T21:30:10.643452Z"},"trusted":true},"outputs":[],"source":["if kaggle_environment:\n","    # To count how many time the \"for loop\" runs.\n","    counter = 0\n","\n","    # init 3 empty lists\n","    test_ls, revealed_targets_ls, sample_prediction_ls = [], [], []\n","\n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","        # Append the dataframe that API return into the list.\n","        test_ls.append(test.copy())\n","        revealed_targets_ls.append(revealed_targets.copy())\n","        sample_prediction_ls.append(sample_prediction.copy())\n","\n","        # Generate features\n","        test = generate_features(test)\n","\n","        # Writes our predictions \n","        sample_prediction[\"target\"] = model.predict(test)\n","        \n","        # This line submit our predictions.\n","        env.predict(sample_prediction)\n","        counter += 1\n","\n","    print('\\n', '=' * 50, sep=\"\")\n","    print(f\"counter: {counter}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
