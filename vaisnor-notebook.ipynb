{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Optiver Trading At The Close\n","\n","[Competition](https://www.kaggle.com/competitions/optiver-trading-at-the-close)\n","\n","[GitHub Repository](https://github.com/cvaisnor/DNN-Final-Project)\n","\n","[Google Slides Presentation](https://docs.google.com/presentation/d/1Xc5F1_NveFi1il3GqHej2aqVmklR_jZU4kx6ZJhiDEM/edit?usp=sharing)"]},{"cell_type":"markdown","metadata":{},"source":["# General Imports"]},{"cell_type":"code","execution_count":135,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.443451Z","iopub.status.busy":"2023-10-17T21:29:46.442668Z","iopub.status.idle":"2023-10-17T21:29:46.826370Z","shell.execute_reply":"2023-10-17T21:29:46.825504Z","shell.execute_reply.started":"2023-10-17T21:29:46.443417Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","kaggle_environment = False # True if running on Kaggle, don't forget to add the dataset!\n","\n","if kaggle_environment:\n","    data_path = '/kaggle/input/'\n","else:\n","    data_path = 'kaggle/input/'\n","\n","# disable PkgResourcesDeprecationWarning\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Setting File Structure"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.829068Z","iopub.status.busy":"2023-10-17T21:29:46.828481Z","iopub.status.idle":"2023-10-17T21:29:46.840764Z","shell.execute_reply":"2023-10-17T21:29:46.839513Z","shell.execute_reply.started":"2023-10-17T21:29:46.829013Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n","kaggle/input/optiver-trading-at-the-close/.gitkeep\n","kaggle/input/optiver-trading-at-the-close/train.csv\n","kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n","kaggle/input/optiver-trading-at-the-close/optiver2023/__pycache__/__init__.cpython-310.pyc\n","kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n","kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n","kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n"]}],"source":["for dirname, _, filenames in os.walk(data_path):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["# Loading Data"]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.843600Z","iopub.status.busy":"2023-10-17T21:29:46.842562Z","iopub.status.idle":"2023-10-17T21:30:06.373150Z","shell.execute_reply":"2023-10-17T21:30:06.371968Z","shell.execute_reply.started":"2023-10-17T21:29:46.843565Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(data_path + 'optiver-trading-at-the-close/train.csv')\n","revealed_targets = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\n","test = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/test.csv')\n","sample_submission = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/sample_submission.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Engineering Section"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","\n","class CustomIterativeImputer(BaseEstimator, TransformerMixin):\n","    def __init__(self, columns, max_iter=10, random_state=0):\n","        self.columns = columns\n","        self.max_iter = max_iter\n","        self.random_state = random_state\n","        self.imputer = None\n","\n","    def fit(self, X, y=None):\n","        # Initialize the IterativeImputer\n","        self.imputer = IterativeImputer(max_iter=self.max_iter, random_state=self.random_state)\n","        self.imputer.fit(X[self.columns])\n","        return self\n","\n","    def transform(self, X):\n","        # Check is fit had been called\n","        if self.imputer is None:\n","            raise RuntimeError(\"You must call fit before calling transform\")\n","        \n","        # Fit the imputer and transform the data\n","        X[self.columns] = self.imputer.transform(X[self.columns])\n","        return X\n","\n","# Create the pipeline with the custom imputer\n","pipeline = Pipeline([\n","    ('iterative_imputer', CustomIterativeImputer(columns=['far_price', 'near_price'])),\n","    # Add other steps here if necessary\n","])"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[],"source":["# Use the pipeline on the training data\n","df_processed = pipeline.fit_transform(train)\n","\n","# drop all rows with NaNs (500-ish rows after imputation)\n","df_processed = df_processed.dropna()\n","\n","# drop row_id column\n","df_processed = df_processed.drop(columns=['row_id'])"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["def add_datetime_feature(df):\n","    \"\"\"\n","    Add datetime features to a DataFrame.\n","\n","    Parameters:\n","    df (pandas.DataFrame): DataFrame to process.\n","\n","    Returns:\n","    pandas.DataFrame: DataFrame with added datetime feature.\n","    \"\"\"\n","    df = df.copy()\n","    start_time = pd.to_timedelta('9:30:00')\n","    # Convert date_id to actual dates starting from 2020-01-01\n","    df['date'] = pd.to_datetime('2020-01-01') + pd.to_timedelta(df['date_id'], unit='D')\n","    df['time'] = pd.to_timedelta(df['time_id'], unit='s')\n","    # Add the trading start time to the time column\n","    df['time'] = df['time'] + start_time\n","    # Create long-form datetime timestamp\n","    df['timestamp'] = df['date'] + df['time']\n","    df.drop(['date', 'time', 'date_id'], axis=1, inplace=True)\n","\n","    # add day\n","    # df['day'] = df['timestamp'].dt.day\n","\n","    # Add unix time\n","    # df['unix_time'] = df['timestamp'].astype(np.int64) // 10**9\n","\n","    # Add a time_idx (an sequence of consecutive integers that goes from min to max date)\n","    # df['time_idx'] = df['timestamp'].rank(method='dense').astype(int) - 1\n","\n","    return df"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[],"source":["df_with_datetime = add_datetime_feature(df_processed)"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[],"source":["def generate_features(test):\n","    return test"]},{"cell_type":"markdown","metadata":{},"source":["# Model Development"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[],"source":["import os\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n","\n","os.chdir(\"../../..\")"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[],"source":["import copy\n","from pathlib import Path\n","import warnings\n","\n","import lightning.pytorch as pl\n","from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n","from lightning.pytorch.loggers import TensorBoardLogger\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n","from pytorch_forecasting.data import GroupNormalizer\n","from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n","from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[],"source":["# imports for Temporal Fusion Transformer\n","\n","import copy\n","from pathlib import Path\n","import warnings\n","import seaborn as sns\n","import matplotlib\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","\n","import numpy as np\n","import pandas as pd\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n","from pytorch_lightning.loggers import TensorBoardLogger\n","import torch\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n","from pytorch_forecasting.data import GroupNormalizer, NaNLabelEncoder\n","from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss, MAE\n","from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n","import random\n","import gc\n","import tensorflow as tf \n","\n","random.seed(30)\n","np.random.seed(30)\n","tf.random.set_seed(30)\n","torch.manual_seed(30)\n","torch.cuda.manual_seed(30)"]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[],"source":["# we want to predict 1 minute ahead of each stock_id\n","max_prediction_length = 60 # (seconds)\n","\n","# max encoder length = number of seconds in 7 hours\n","max_encoder_length = 7 * 60 * 60 # (seconds)"]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[],"source":["# get min and max datetimes in the dataset\n","min_time = df_with_datetime['timestamp'].min()\n","max_time = df_with_datetime['timestamp'].max()\n","\n","# training cutoff will be the last 20 percent of the data\n","cutoff = min_time + (max_time - min_time) * 0.8"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[],"source":["# convert \"stock_id\" to string so it is categorical\n","df_with_datetime['stock_id'] = df_with_datetime['stock_id'].astype(str)"]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[],"source":["# creating the dataset\n","training = TimeSeriesDataSet(\n","    df_with_datetime[df_with_datetime['timestamp'] <= cutoff],\n","    time_idx=\"time_id\",\n","    target=\"target\",\n","    group_ids=[\"stock_id\"],\n","    min_encoder_length=0,  # allow predictions without history\n","    max_encoder_length=max_encoder_length,\n","    min_prediction_length=1,\n","    max_prediction_length=max_prediction_length,\n","    static_categoricals=[\"stock_id\"],\n","    time_varying_unknown_reals=[\"target\"],\n","    allow_missing_timesteps=True,\n",")"]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[],"source":["# TypeError: Addition/subtraction of integers and integer-arrays with Timestamp is no longer supported.  Instead of adding/subtracting `n`, use `n * obj.freq`\n","\n","# create validation set (predict=True) which means to predict the last max_prediction_length points in time for each series\n","validation = TimeSeriesDataSet.from_dataset(training, df_with_datetime, predict=True, stop_randomization=True)"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[],"source":["# creating dataloaders\n","batch_size = 128  # set this between 32 to 128\n","train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n","val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Global seed set to 42\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["Number of parameters in network: 6.2k\n"]}],"source":["PATIENCE = 30\n","MAX_EPOCHS = 120\n","LEARNING_RATE = 0.03\n","OPTUNA = False\n","\n","early_stopping_callback = EarlyStopping(monitor=\"mae\", min_delta=1e-4, patience=PATIENCE, verbose=False, mode=\"min\")\n","\n","lr_logger = LearningRateMonitor()  # log the learning rate\n","\n","# trainer = pl.Trainer(\n","#     max_epochs=MAX_EPOCHS,\n","#     gpus=1,\n","#     accelerator=\"gpu\",\n","#     enable_model_summary=True,\n","#     gradient_clip_val=0.1,\n","#     limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n","#     fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n","#     callbacks=[lr_logger, early_stopping_callback],\n","# )\n","\n","# configure network and trainer\n","pl.seed_everything(42)\n","trainer = pl.Trainer(\n","    accelerator=\"gpu\",\n","    # clipping gradients is a hyperparameter and important to prevent divergance\n","    # of the gradient for recurrent neural networks\n","    gradient_clip_val=0.1,\n",")\n","\n","tft = TemporalFusionTransformer.from_dataset(\n","    training,\n","    # not meaningful for finding the learning rate but otherwise very important\n","    learning_rate=0.03,\n","    hidden_size=8,  # most important hyperparameter apart from learning rate\n","    # number of attention heads. Set to up to 4 for large datasets\n","    attention_head_size=1,\n","    dropout=0.1,  # between 0.1 and 0.3 are good values\n","    hidden_continuous_size=8,  # set to <= hidden_size\n","    loss=QuantileLoss(),\n","    optimizer=\"Ranger\"\n","    # reduce learning rate if no improvement in validation loss after x epochs\n","    # reduce_on_plateau_patience=1000,\n",")\n","print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24176/1479756326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m res = Tuner(trainer).lr_find(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/tuner/tuning.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlr_finder_callback\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlr_finder_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mdatamodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAn\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningDataModule\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \"\"\"\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_unwrap_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         call._call_and_handle_interrupt(\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;34mf\"`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `{type(model).__qualname__}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         )\n","\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`"]}],"source":["# find optimal learning rate\n","from lightning.pytorch.tuner import Tuner\n","\n","res = Tuner(trainer).lr_find(\n","    tft,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n","    max_lr=10.0,\n","    min_lr=1e-6,\n",")\n","\n","print(f\"suggested learning rate: {res.suggestion()}\")\n","fig = res.plot(show=True, suggest=True)\n","fig.show()"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters in network: 18.6k\n"]}],"source":["tft = TemporalFusionTransformer.from_dataset(\n","    training,\n","    learning_rate=LEARNING_RATE,\n","    hidden_size=16,  # biggest influence network size\n","    attention_head_size=1,\n","    dropout=0.1,\n","    hidden_continuous_size=8,\n","    output_size=7,  # QuantileLoss has 7 quantiles by default\n","    loss=QuantileLoss(),\n","    # reduce learning rate if no improvement in validation loss after x epochs\n","    reduce_on_plateau_patience=4,\n",")\n","\n","print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n","\n","# t"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Missing logger folder: /home/chris/dev/jhu/605.742_deep_neural_networks/final_project/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"text/plain":["tensor(6.0678, device='cuda:0')"]},"execution_count":159,"metadata":{},"output_type":"execute_result"}],"source":["# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n","baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n","MAE()(baseline_predictions.output, baseline_predictions.y)"]},{"cell_type":"markdown","metadata":{},"source":["# Generating Predictions\n","\n","Only for Kaggle:\n","When uploading notebook, make sure to add the Optiver data on the right side of the screen."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.613776Z","iopub.status.busy":"2023-10-17T21:30:10.613395Z","iopub.status.idle":"2023-10-17T21:30:10.641355Z","shell.execute_reply":"2023-10-17T21:30:10.640398Z","shell.execute_reply.started":"2023-10-17T21:30:10.613732Z"},"trusted":true},"outputs":[],"source":["if kaggle_environment:\n","    import optiver2023\n","    env = optiver2023.make_env()\n","    iter_test = env.iter_test()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.643482Z","iopub.status.busy":"2023-10-17T21:30:10.642850Z","iopub.status.idle":"2023-10-17T21:30:11.480530Z","shell.execute_reply":"2023-10-17T21:30:11.479410Z","shell.execute_reply.started":"2023-10-17T21:30:10.643452Z"},"trusted":true},"outputs":[],"source":["if kaggle_environment:\n","    # To count how many time the \"for loop\" runs.\n","    counter = 0\n","\n","    # init 3 empty lists\n","    test_ls, revealed_targets_ls, sample_prediction_ls = [], [], []\n","\n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","        # Append the dataframe that API return into the list.\n","        test_ls.append(test.copy())\n","        revealed_targets_ls.append(revealed_targets.copy())\n","        sample_prediction_ls.append(sample_prediction.copy())\n","\n","        # Generate features\n","        test = generate_features(test)\n","\n","        # Writes our predictions \n","        sample_prediction[\"target\"] = model.predict(test)\n","        \n","        # This line submit our predictions.\n","        env.predict(sample_prediction)\n","        counter += 1\n","\n","    print('\\n', '=' * 50, sep=\"\")\n","    print(f\"counter: {counter}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
