{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Optiver Trading At The Close\n","\n","[Competition](https://www.kaggle.com/competitions/optiver-trading-at-the-close)\n","\n","[GitHub Repository](https://github.com/cvaisnor/DNN-Final-Project)\n","\n","[Google Slides Presentation](https://docs.google.com/presentation/d/1Xc5F1_NveFi1il3GqHej2aqVmklR_jZU4kx6ZJhiDEM/edit?usp=sharing)"]},{"cell_type":"markdown","metadata":{},"source":["# General Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.443451Z","iopub.status.busy":"2023-10-17T21:29:46.442668Z","iopub.status.idle":"2023-10-17T21:29:46.826370Z","shell.execute_reply":"2023-10-17T21:29:46.825504Z","shell.execute_reply.started":"2023-10-17T21:29:46.443417Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","kaggle_environment = True # True if running on Kaggle, don't forget to add the dataset!\n","\n","if kaggle_environment:\n","    data_path = '/kaggle/input/'\n","else:\n","    data_path = 'kaggle/input/'"]},{"cell_type":"markdown","metadata":{},"source":["# Setting File Structure"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.829068Z","iopub.status.busy":"2023-10-17T21:29:46.828481Z","iopub.status.idle":"2023-10-17T21:29:46.840764Z","shell.execute_reply":"2023-10-17T21:29:46.839513Z","shell.execute_reply.started":"2023-10-17T21:29:46.829013Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n","/kaggle/input/optiver-trading-at-the-close/train.csv\n","/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n","/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n","/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n","/kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n","/kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n"]}],"source":["for dirname, _, filenames in os.walk(data_path):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["# Loading Data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:29:46.843600Z","iopub.status.busy":"2023-10-17T21:29:46.842562Z","iopub.status.idle":"2023-10-17T21:30:06.373150Z","shell.execute_reply":"2023-10-17T21:30:06.371968Z","shell.execute_reply.started":"2023-10-17T21:29:46.843565Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv(data_path + 'optiver-trading-at-the-close/train.csv')\n","revealed_targets = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/revealed_targets.csv')\n","test = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/test.csv')\n","sample_submission = pd.read_csv(data_path + 'optiver-trading-at-the-close/example_test_files/sample_submission.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Engineering Section"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:06.375802Z","iopub.status.busy":"2023-10-17T21:30:06.374676Z","iopub.status.idle":"2023-10-17T21:30:06.380310Z","shell.execute_reply":"2023-10-17T21:30:06.379187Z","shell.execute_reply.started":"2023-10-17T21:30:06.375765Z"},"trusted":true},"outputs":[],"source":["# imports"]},{"cell_type":"markdown","metadata":{},"source":["All feature transformations should be combined into a generate_feature() function. This can then be applied to the test data will in the for loop for the Optiver API."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:06.382936Z","iopub.status.busy":"2023-10-17T21:30:06.382593Z","iopub.status.idle":"2023-10-17T21:30:06.396015Z","shell.execute_reply":"2023-10-17T21:30:06.394500Z","shell.execute_reply.started":"2023-10-17T21:30:06.382907Z"},"trusted":true},"outputs":[],"source":["def generate_features(test):\n","    return test"]},{"cell_type":"markdown","metadata":{},"source":["# Slicing\n","\n","Generally, don't change the cell below. It is used to slice the data into training and validation sets."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:06.397849Z","iopub.status.busy":"2023-10-17T21:30:06.397370Z","iopub.status.idle":"2023-10-17T21:30:10.587219Z","shell.execute_reply":"2023-10-17T21:30:10.586217Z","shell.execute_reply.started":"2023-10-17T21:30:06.397816Z"},"trusted":true},"outputs":[],"source":["# imports\n","from sklearn.model_selection import train_test_split\n","\n","y = train['target'].values\n","X = train.drop(columns='target')\n","\n","# train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Development\n","\n","From: https://github.com/davidrpugh/stochastic-expatriate-descent/blob/2020-04-11-double-dqn/_notebooks/2020-04-11-double-dqn.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","\n","def select_greedy_actions(states: torch.Tensor, q_network: nn.Module) -> torch.Tensor:\n","    \"\"\"Select the greedy action for the current state given some Q-network.\"\"\"\n","    _, actions = q_network(states).max(dim=1, keepdim=True)\n","    return actions\n","\n","\n","def evaluate_selected_actions(states: torch.Tensor,\n","                              actions: torch.Tensor,\n","                              rewards: torch.Tensor,\n","                              dones: torch.Tensor,\n","                              gamma: float,\n","                              q_network: nn.Module) -> torch.Tensor:\n","    \"\"\"Compute the Q-values by evaluating the actions given the current states and Q-network.\"\"\"\n","    next_q_values = q_network(states).gather(dim=1, index=actions)        \n","    q_values = rewards + (gamma * next_q_values * (1 - dones))\n","    return q_values\n","\n","\n","def q_learning_update(states: torch.Tensor,\n","                      rewards: torch.Tensor,\n","                      dones: torch.Tensor,\n","                      gamma: float,\n","                      q_network: nn.Module) -> torch.Tensor:\n","    \"\"\"Q-Learning update with explicitly decoupled action selection and evaluation steps.\"\"\"\n","    actions = select_greedy_actions(states, q_network)\n","    q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q_network)\n","    return q_values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def double_q_learning_update(states: torch.Tensor,\n","                             rewards: torch.Tensor,\n","                             dones: torch.Tensor,\n","                             gamma: float,\n","                             q_network_1: nn.Module,\n","                             q_network_2: nn.Module) -> torch.Tensor:\n","    \"\"\"Double Q-Learning uses Q-network 1 to select actions and Q-network 2 to evaluate the selected actions.\"\"\"\n","    actions = select_greedy_actions(states, q_network_1)\n","    q_values = evaluate_selected_actions(states, actions, rewards, dones, gamma, q_network_2)\n","    return q_values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import collections\n","import typing\n","\n","import numpy as np\n","\n","\n","_field_names = [\n","    \"state\",\n","    \"action\",\n","    \"reward\",\n","    \"next_state\",\n","    \"done\"\n","]\n","Experience = collections.namedtuple(\"Experience\", field_names=_field_names)\n","\n","\n","class ExperienceReplayBuffer:\n","    \"\"\"Fixed-size buffer to store Experience tuples.\"\"\"\n","\n","    def __init__(self,\n","                 batch_size: int,\n","                 buffer_size: int = None,\n","                 random_state: np.random.RandomState = None) -> None:\n","        \"\"\"\n","        Initialize an ExperienceReplayBuffer object.\n","\n","        Parameters:\n","        -----------\n","        buffer_size (int): maximum size of buffer\n","        batch_size (int): size of each training batch\n","        random_state (np.random.RandomState): random number generator.\n","        \n","        \"\"\"\n","        self._batch_size = batch_size\n","        self._buffer_size = buffer_size\n","        self._buffer = collections.deque(maxlen=buffer_size)\n","        self._random_state = np.random.RandomState() if random_state is None else random_state\n","        \n","    def __len__(self) -> int:\n","        return len(self._buffer)\n","    \n","    @property\n","    def batch_size(self) -> int:\n","        \"\"\"Number of experience samples per training batch.\"\"\"\n","        return self._batch_size\n","    \n","    @property\n","    def buffer_size(self) -> int:\n","        \"\"\"Total number of experience samples stored in memory.\"\"\"\n","        return self._buffer_size\n","    \n","    def append(self, experience: Experience) -> None:\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        self._buffer.append(experience)\n","    \n","    def sample(self) -> typing.List[Experience]:\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        idxs = self._random_state.randint(len(self._buffer), size=self._batch_size)\n","        experiences = [self._buffer[idx] for idx in idxs]\n","        return experiences"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import typing\n","\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","\n","\n","class Agent:\n","    \n","    def choose_action(self, state: np.array) -> int:\n","        \"\"\"Rule for choosing an action given the current state of the environment.\"\"\"\n","        raise NotImplementedError\n","        \n","    def learn(self, experiences: typing.List[Experience]) -> None:\n","        \"\"\"Update the agent's state based on a collection of recent experiences.\"\"\"\n","        raise NotImplementedError\n","\n","    def save(self, filepath) -> None:\n","        \"\"\"Save any important agent state to a file.\"\"\"\n","        raise NotImplementedError\n","        \n","    def step(self,\n","             state: np.array,\n","             action: int,\n","             reward: float,\n","             next_state: np.array,\n","             done: bool) -> None:\n","        \"\"\"Update agent's state after observing the effect of its action on the environment.\"\"\"\n","        raise NotImplmentedError\n","\n","\n","class DeepQAgent(Agent):\n","\n","    def __init__(self,\n","                 state_size: int,\n","                 action_size: int,\n","                 number_hidden_units: int,\n","                 optimizer_fn: typing.Callable[[typing.Iterable[nn.Parameter]], optim.Optimizer],\n","                 batch_size: int,\n","                 buffer_size: int,\n","                 epsilon_decay_schedule: typing.Callable[[int], float],\n","                 alpha: float,\n","                 gamma: float,\n","                 update_frequency: int,\n","                 double_dqn: bool = False,\n","                 seed: int = None) -> None:\n","        \"\"\"\n","        Initialize a DeepQAgent.\n","        \n","        Parameters:\n","        -----------\n","        state_size (int): the size of the state space.\n","        action_size (int): the size of the action space.\n","        number_hidden_units (int): number of units in the hidden layers.\n","        optimizer_fn (callable): function that takes Q-network parameters and returns an optimizer.\n","        batch_size (int): number of experience tuples in each mini-batch.\n","        buffer_size (int): maximum number of experience tuples stored in the replay buffer.\n","        epsilon_decay_schdule (callable): function that takes episode number and returns epsilon.\n","        alpha (float): rate at which the target q-network parameters are updated.\n","        gamma (float): Controls how much that agent discounts future rewards (0 < gamma <= 1).\n","        update_frequency (int): frequency (measured in time steps) with which q-network parameters are updated.\n","        double_dqn (bool): whether to use vanilla DQN algorithm or use the Double DQN algorithm.\n","        seed (int): random seed\n","        \n","        \"\"\"\n","        self._state_size = state_size\n","        self._action_size = action_size\n","        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        \n","        # set seeds for reproducibility\n","        self._random_state = np.random.RandomState() if seed is None else np.random.RandomState(seed)\n","        if seed is not None:\n","            torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.backends.cudnn.deterministic = True\n","            torch.backends.cudnn.benchmark = False\n","        \n","        # initialize agent hyperparameters\n","        _replay_buffer_kwargs = {\n","            \"batch_size\": batch_size,\n","            \"buffer_size\": buffer_size,\n","            \"random_state\": self._random_state\n","        }\n","        self._memory = ExperienceReplayBuffer(**_replay_buffer_kwargs)\n","        self._epsilon_decay_schedule = epsilon_decay_schedule\n","        self._alpha = alpha\n","        self._gamma = gamma\n","        self._double_dqn = double_dqn\n","        \n","        # initialize Q-Networks\n","        self._update_frequency = update_frequency\n","        self._online_q_network = self._initialize_q_network(number_hidden_units)\n","        self._target_q_network = self._initialize_q_network(number_hidden_units)\n","        self._synchronize_q_networks(self._target_q_network, self._online_q_network)        \n","        self._online_q_network.to(self._device)\n","        self._target_q_network.to(self._device)\n","        \n","        # initialize the optimizer\n","        self._optimizer = optimizer_fn(self._online_q_network.parameters())\n","\n","        # initialize some counters\n","        self._number_episodes = 0\n","        self._number_timesteps = 0\n","        \n","    def _initialize_q_network(self, number_hidden_units: int) -> nn.Module:\n","        \"\"\"Create a neural network for approximating the action-value function.\"\"\"\n","        q_network = nn.Sequential(\n","            nn.Linear(in_features=self._state_size, out_features=number_hidden_units),\n","            nn.ReLU(),\n","            nn.Linear(in_features=number_hidden_units, out_features=number_hidden_units),\n","            nn.ReLU(),\n","            nn.Linear(in_features=number_hidden_units, out_features=self._action_size)\n","        )\n","        return q_network\n","                 \n","    @staticmethod\n","    def _soft_update_q_network_parameters(q_network_1: nn.Module,\n","                                          q_network_2: nn.Module,\n","                                          alpha: float) -> None:\n","        \"\"\"In-place, soft-update of q_network_1 parameters with parameters from q_network_2.\"\"\"\n","        for p1, p2 in zip(q_network_1.parameters(), q_network_2.parameters()):\n","            p1.data.copy_(alpha * p2.data + (1 - alpha) * p1.data)\n","    \n","    @staticmethod\n","    def _synchronize_q_networks(q_network_1: nn.Module, q_network_2: nn.Module) -> None:\n","        \"\"\"In place, synchronization of q_network_1 and q_network_2.\"\"\"\n","        _ = q_network_1.load_state_dict(q_network_2.state_dict())\n","           \n","    def _uniform_random_policy(self, state: torch.Tensor) -> int:\n","        \"\"\"Choose an action uniformly at random.\"\"\"\n","        return self._random_state.randint(self._action_size)\n","        \n","    def _greedy_policy(self, state: torch.Tensor) -> int:\n","        \"\"\"Choose an action that maximizes the action_values given the current state.\"\"\"\n","        action = (self._online_q_network(state)\n","                      .argmax()\n","                      .cpu()  # action_values might reside on the GPU!\n","                      .item())\n","        return action\n","    \n","    def _epsilon_greedy_policy(self, state: torch.Tensor, epsilon: float) -> int:\n","        \"\"\"With probability epsilon explore randomly; otherwise exploit knowledge optimally.\"\"\"\n","        if self._random_state.random() < epsilon:\n","            action = self._uniform_random_policy(state)\n","        else:\n","            action = self._greedy_policy(state)\n","        return action\n","\n","    def choose_action(self, state: np.array) -> int:\n","        \"\"\"\n","        Return the action for given state as per current policy.\n","        \n","        Parameters:\n","        -----------\n","        state (np.array): current state of the environment.\n","        \n","        Return:\n","        --------\n","        action (int): an integer representing the chosen action.\n","\n","        \"\"\"\n","        # need to reshape state array and convert to tensor\n","        state_tensor = (torch.from_numpy(state)\n","                             .unsqueeze(dim=0)\n","                             .to(self._device))\n","            \n","        # choose uniform at random if agent has insufficient experience\n","        if not self.has_sufficient_experience():\n","            action = self._uniform_random_policy(state_tensor)\n","        else:\n","            epsilon = self._epsilon_decay_schedule(self._number_episodes)\n","            action = self._epsilon_greedy_policy(state_tensor, epsilon)\n","        return action\n","    \n","    def learn(self, experiences: typing.List[Experience]) -> None:\n","        \"\"\"Update the agent's state based on a collection of recent experiences.\"\"\"\n","        states, actions, rewards, next_states, dones = (torch.Tensor(vs).to(self._device) for vs in zip(*experiences))\n","        \n","        # need to add second dimension to some tensors\n","        actions = (actions.long()\n","                          .unsqueeze(dim=1))\n","        rewards = rewards.unsqueeze(dim=1)\n","        dones = dones.unsqueeze(dim=1)\n","        \n","        if self._double_dqn:\n","            target_q_values = double_q_learning_update(next_states,\n","                                                       rewards,\n","                                                       dones,\n","                                                       self._gamma,\n","                                                       self._online_q_network,\n","                                                       self._target_q_network)\n","        else:\n","            target_q_values = q_learning_update(next_states,\n","                                                rewards,\n","                                                dones,\n","                                                self._gamma,\n","                                                self._target_q_network)\n","\n","        online_q_values = (self._online_q_network(states)\n","                               .gather(dim=1, index=actions))\n","        \n","        # compute the mean squared loss\n","        loss = F.mse_loss(online_q_values, target_q_values)\n","        \n","        # updates the parameters of the online network\n","        self._optimizer.zero_grad()\n","        loss.backward()\n","        self._optimizer.step()\n","        \n","        self._soft_update_q_network_parameters(self._target_q_network,\n","                                               self._online_q_network,\n","                                               self._alpha)\n","    \n","    def has_sufficient_experience(self) -> bool:\n","        \"\"\"True if agent has enough experience to train on a batch of samples; False otherwise.\"\"\"\n","        return len(self._memory) >= self._memory.batch_size\n","    \n","    def save(self, filepath: str) -> None:\n","        \"\"\"\n","        Saves the state of the DeepQAgent.\n","        \n","        Parameters:\n","        -----------\n","        filepath (str): filepath where the serialized state should be saved.\n","        \n","        Notes:\n","        ------\n","        The method uses `torch.save` to serialize the state of the q-network, \n","        the optimizer, as well as the dictionary of agent hyperparameters.\n","        \n","        \"\"\"\n","        checkpoint = {\n","            \"q-network-state\": self._online_q_network.state_dict(),\n","            \"optimizer-state\": self._optimizer.state_dict(),\n","            \"agent-hyperparameters\": {\n","                \"alpha\": self._alpha,\n","                \"batch_size\": self._memory.batch_size,\n","                \"buffer_size\": self._memory.buffer_size,\n","                \"gamma\": self._gamma,\n","                \"update_frequency\": self._update_frequency\n","            }\n","        }\n","        torch.save(checkpoint, filepath)\n","        \n","    def step(self,\n","             state: np.array,\n","             action: int,\n","             reward: float,\n","             next_state: np.array,\n","             done: bool) -> None:\n","        \"\"\"\n","        Updates the agent's state based on feedback received from the environment.\n","        \n","        Parameters:\n","        -----------\n","        state (np.array): the previous state of the environment.\n","        action (int): the action taken by the agent in the previous state.\n","        reward (float): the reward received from the environment.\n","        next_state (np.array): the resulting state of the environment following the action.\n","        done (bool): True is the training episode is finised; false otherwise.\n","        \n","        \"\"\"\n","        experience = Experience(state, action, reward, next_state, done)\n","        self._memory.append(experience)\n","            \n","        if done:\n","            self._number_episodes += 1\n","        else:\n","            self._number_timesteps += 1\n","            \n","            # every so often the agent should learn from experiences\n","            if self._number_timesteps % self._update_frequency == 0 and self.has_sufficient_experience():\n","                experiences = self._memory.sample()\n","                self.learn(experiences)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import collections\n","import typing\n","\n","import gym\n","\n","\n","def _train_for_at_most(agent: Agent, env: gym.Env, max_timesteps: int) -> int:\n","    \"\"\"Train agent for a maximum number of timesteps.\"\"\"\n","    state = env.reset()\n","    score = 0\n","    for t in range(max_timesteps):\n","        action = agent.choose_action(state)\n","        next_state, reward, done, _ = env.step(action)\n","        agent.step(state, action, reward, next_state, done)\n","        state = next_state\n","        score += reward\n","        if done:\n","            break\n","    return score\n","\n","                \n","def _train_until_done(agent: Agent, env: gym.Env) -> float:\n","    \"\"\"Train the agent until the current episode is complete.\"\"\"\n","    state = env.reset()\n","    score = 0\n","    done = False\n","    while not done:\n","        action = agent.choose_action(state)\n","        next_state, reward, done, _ = env.step(action)\n","        agent.step(state, action, reward, next_state, done)\n","        state = next_state\n","        score += reward\n","    return score\n","\n","\n","def train(agent: Agent,\n","          env: gym.Env,\n","          checkpoint_filepath: str,\n","          target_score: float,\n","          number_episodes: int,\n","          maximum_timesteps=None) -> typing.List[float]:\n","    \"\"\"\n","    Reinforcement learning training loop.\n","    \n","    Parameters:\n","    -----------\n","    agent (Agent): an agent to train.\n","    env (gym.Env): an environment in which to train the agent.\n","    checkpoint_filepath (str): filepath used to save the state of the trained agent.\n","    number_episodes (int): maximum number of training episodes.\n","    maximum_timsteps (int): maximum number of timesteps per episode.\n","    \n","    Returns:\n","    --------\n","    scores (list): collection of episode scores from training.\n","    \n","    \"\"\"\n","    scores = []\n","    most_recent_scores = collections.deque(maxlen=100)\n","    for i in range(number_episodes):\n","        if maximum_timesteps is None:\n","            score = _train_until_done(agent, env)\n","        else:\n","            score = _train_for_at_most(agent, env, maximum_timesteps)         \n","        scores.append(score)\n","        most_recent_scores.append(score)\n","        \n","        average_score = sum(most_recent_scores) / len(most_recent_scores)\n","        if average_score >= target_score:\n","            print(f\"\\nEnvironment solved in {i:d} episodes!\\tAverage Score: {average_score:.2f}\")\n","            agent.save(checkpoint_filepath)\n","            break\n","        if (i + 1) % 100 == 0:\n","            print(f\"\\rEpisode {i + 1}\\tAverage Score: {average_score:.2f}\")\n","\n","    return scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def power_decay_schedule(episode_number: int,\n","                         decay_factor: float,\n","                         minimum_epsilon: float) -> float:\n","    \"\"\"Power decay schedule found in other practical applications.\"\"\"\n","    return max(decay_factor**episode_number, minimum_epsilon)\n","\n","_epsilon_decay_schedule_kwargs = {\n","    \"decay_factor\": 0.99,\n","    \"minimum_epsilon\": 1e-2,\n","}\n","epsilon_decay_schedule = lambda n: power_decay_schedule(n, **_epsilon_decay_schedule_kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch import optim\n","\n","_optimizer_kwargs = {\n","    \"lr\": 1e-3,\n","    \"betas\": (0.9, 0.999),\n","    \"eps\": 1e-08,\n","    \"weight_decay\": 0,\n","    \"amsgrad\": False,\n","}\n","optimizer_fn = lambda parameters: optim.Adam(parameters, **_optimizer_kwargs)"]},{"cell_type":"markdown","metadata":{},"source":["# Double Deep Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_agent_kwargs = {\n","    \"state_size\": env.observation_space.shape[0],\n","    \"action_size\": env.action_space.n, \n","    \"number_hidden_units\": 64,\n","    \"optimizer_fn\": optimizer_fn,\n","    \"epsilon_decay_schedule\": epsilon_decay_schedule,\n","    \"batch_size\": 64,\n","    \"buffer_size\": 100000,\n","    \"alpha\": 1e-3,\n","    \"gamma\": 0.99,\n","    \"update_frequency\": 4,\n","    \"double_dqn\": True,  # True uses Double DQN; False uses DQN \n","    \"seed\": None,\n","}\n","double_dqn_agent = DeepQAgent(**_agent_kwargs)\n","\n","double_dqn_scores = train(double_dqn_agent,\n","                          env,\n","                          \"double-dqn-checkpoint.pth\",\n","                          number_episodes=2000,\n","                          target_score=200)"]},{"cell_type":"markdown","metadata":{},"source":["# Regular Deep Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_agent_kwargs = {\n","    \"state_size\": env.observation_space.shape[0],\n","    \"action_size\": env.action_space.n, \n","    \"number_hidden_units\": 64,\n","    \"optimizer_fn\": optimizer_fn,\n","    \"epsilon_decay_schedule\": epsilon_decay_schedule,\n","    \"batch_size\": 64,\n","    \"buffer_size\": 100000,\n","    \"alpha\": 1e-3,\n","    \"gamma\": 0.99,\n","    \"update_frequency\": 4,\n","    \"double_dqn\": False,  # True uses Double DQN; False uses DQN \n","    \"seed\": None,\n","}\n","dqn_agent = DeepQAgent(**_agent_kwargs)\n","\n","dqn_scores = train(dqn_agent,\n","                   env,\n","                   \"dqn-checkpoint.pth\",\n","                   number_episodes=2000,\n","                   target_score=200)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.589632Z","iopub.status.busy":"2023-10-17T21:30:10.588439Z","iopub.status.idle":"2023-10-17T21:30:10.594763Z","shell.execute_reply":"2023-10-17T21:30:10.593758Z","shell.execute_reply.started":"2023-10-17T21:30:10.589587Z"},"trusted":true},"outputs":[],"source":["# imports\n","\n","class Model:\n","    def __init__(self) -> None:\n","        pass\n","\n","    def predict(self, X):\n","        return np.random.rand(len(X))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.596968Z","iopub.status.busy":"2023-10-17T21:30:10.596476Z","iopub.status.idle":"2023-10-17T21:30:10.611827Z","shell.execute_reply":"2023-10-17T21:30:10.610479Z","shell.execute_reply.started":"2023-10-17T21:30:10.596930Z"},"trusted":true},"outputs":[],"source":["model = Model()"]},{"cell_type":"markdown","metadata":{},"source":["# Generating Predictions\n","\n","Only for Kaggle:\n","When uploading notebook, make sure to add the Optiver data on the right side of the screen."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.613776Z","iopub.status.busy":"2023-10-17T21:30:10.613395Z","iopub.status.idle":"2023-10-17T21:30:10.641355Z","shell.execute_reply":"2023-10-17T21:30:10.640398Z","shell.execute_reply.started":"2023-10-17T21:30:10.613732Z"},"trusted":true},"outputs":[],"source":["if kaggle_environment:\n","    import optiver2023\n","    env = optiver2023.make_env()\n","    iter_test = env.iter_test()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:30:10.643482Z","iopub.status.busy":"2023-10-17T21:30:10.642850Z","iopub.status.idle":"2023-10-17T21:30:11.480530Z","shell.execute_reply":"2023-10-17T21:30:11.479410Z","shell.execute_reply.started":"2023-10-17T21:30:10.643452Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","\n","==================================================\n","counter: 165\n"]}],"source":["if kaggle_environment:\n","    # To count how many time the \"for loop\" runs.\n","    counter = 0\n","\n","    # init 3 empty lists\n","    test_ls, revealed_targets_ls, sample_prediction_ls = [], [], []\n","\n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","        # Append the dataframe that API return into the list.\n","        test_ls.append(test.copy())\n","        revealed_targets_ls.append(revealed_targets.copy())\n","        sample_prediction_ls.append(sample_prediction.copy())\n","\n","        # Generate features\n","        test = generate_features(test)\n","\n","        # Writes our predictions \n","        sample_prediction[\"target\"] = model.predict(test)\n","        \n","        # This line submit our predictions.\n","        env.predict(sample_prediction)\n","        counter += 1\n","\n","    print('\\n', '=' * 50, sep=\"\")\n","    print(f\"counter: {counter}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Setting up stock trading as a reinforcement learning problem"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
